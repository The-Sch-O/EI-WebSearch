{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/The-Sch-O/EI-WebSearch/blob/main/Search_Engine_with_neurons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKmLXLVA81au"
      },
      "source": [
        "# Final Notebook\n",
        "\n",
        "This notebook is your search engine. \n",
        "\n",
        "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqjgWm6r81ax"
      },
      "source": [
        "## Initialisation\n",
        "\n",
        "- Install libraries (if you use Colab and needed),\n",
        "- Import the modules,\n",
        "- Declare global variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MGtD9tkJ81ax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c62651-e1a1-4837-8769-e828b6e95929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting py7zr\n",
            "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable (from py7zr)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n",
            "  Downloading pyzstd-0.15.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
            "Successfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.7 texttable-1.6.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ttable\n",
            "  Downloading ttable-0.6.4.tar.gz (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.3/122.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ttable\n",
            "  Building wheel for ttable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ttable: filename=ttable-0.6.4-cp310-cp310-linux_x86_64.whl size=212615 sha256=9aa11502af4542dc4233d2229667e57411230fd43e3fde1c34553aa4e160e469\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/8d/56/f2572fdbf1ef1f8a947d7ff25ce18d9373d8e02a68f9ac8de6\n",
            "Successfully built ttable\n",
            "Installing collected packages: ttable\n",
            "Successfully installed ttable-0.6.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=dcfaf1b2c85d16bb5d1cc46e98173de7d30a72fea2c58fbcd1a16ffd47d18ea4\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.14.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk\n",
        "! pip install py7zr\n",
        "! pip install ttable\n",
        "! pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-UIJ_Aq81ay"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "import math\n",
        "import py7zr\n",
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from math import log\n",
        "from tt import BooleanExpression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('all')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "stops = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AhSE9co81az"
      },
      "source": [
        "On google colab use this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoVT37Iz81az"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "MAIN_PATH = '/content/drive/MyDrive/TP Centrale'\n",
        "DATA_PATH = '/content/drive/MyDrive/TP Centrale/data'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NRMStjt81az"
      },
      "source": [
        "And in VS Code use this :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TInsTsg81az"
      },
      "outputs": [],
      "source": [
        "# MAIN_PATH = ''\n",
        "# DATA_PATH = '/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNFb3hkZ81a0"
      },
      "outputs": [],
      "source": [
        "INVINDEX_PATH = os.path.join(DATA_PATH, \"inverted_index.pickle\")\n",
        "EMBEDDING_PATH = os.path.join(DATA_PATH, \"embeddings.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ieUQ5181a0"
      },
      "source": [
        "## Extraction the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuG8YOpw81a0"
      },
      "outputs": [],
      "source": [
        "def extract_data(filepath):\n",
        "    if not os.path.isdir(MAIN_PATH):\n",
        "        os.mkdir(MAIN_PATH)\n",
        "    if not os.path.isdir(MAIN_PATH):\n",
        "        os.mkdir(DATA_PATH)\n",
        "    archive = py7zr.SevenZipFile(os.path.join(MAIN_PATH, 'datascience.stackexchange.com.7z'), mode='r')\n",
        "    archive.extractall(path=os.path.join(MAIN_PATH, 'data'))\n",
        "    archive.close()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9-1FLK181a0"
      },
      "outputs": [],
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azPIww8r81a1"
      },
      "source": [
        "## Indexation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGNHjDxo81a1"
      },
      "source": [
        "def index_data():\n",
        "    # TODO\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl8tw9pY81a1"
      },
      "outputs": [],
      "source": [
        "def extract_words(text:str)->list:\n",
        "  \"\"\"Transforms a given text into a list of tokens\"\"\"\n",
        "  tokens = text.lower()\n",
        "  tokens = nltk.tokenize.word_tokenize(tokens)\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i].rstrip(\".!?,;:\\(\\)\\\"\\'\")\n",
        "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def remove_tags(text: str) -> str:\n",
        "    \"\"\"Remove the HTML tags from a given text\"\"\"\n",
        "    cleaned_text = re.sub(r'<.*?>', ' ', text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespaces\n",
        "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing whitespaces\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def filter_stop_words(words:list[str]) -> list[str]:\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    if word not in stops:\n",
        "        new_words.append(word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def clean_post(text:str):\n",
        "  text = remove_tags(text)\n",
        "  text = text.lower()\n",
        "  res = nltk.word_tokenize(text)\n",
        "  clean = ''\n",
        "  for i in range(len(res)):\n",
        "    res[i] = lemmatizer.lemmatize(res[i])\n",
        "    if res[i] not in stops and res[i] not in string.punctuation:\n",
        "      clean += str(res[i]) + ' '\n",
        "  return clean\n",
        "\n",
        "\n",
        "def inverted_index_data():\n",
        "    # TODO\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMSrS4iv81a1"
      },
      "outputs": [],
      "source": [
        "clean_posts = posts[['Id','Body']]\n",
        "clean_posts['Words'] = clean_posts['Body'].fillna('').apply(remove_tags).apply(extract_words).apply(filter_stop_words)\n",
        "clean_posts['len'] = clean_posts['Words'].apply(len) #On a besoin de cette donnée en accès rapide pour OBM25\n",
        "posts['Cleaned_post'] = posts['Body'].fillna('').apply(clean_post)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WRaEtOC81a2"
      },
      "outputs": [],
      "source": [
        "def create_inverted_index(posts:pd.DataFrame)->dict:\n",
        "  \"\"\"\n",
        "  On suppose que les posts sont pré-traités. \n",
        "  On va renvoyer un index inversé complet et un index des TF\n",
        "  full_ind = {key : {'df' : int , 'inv_ind' : [ (id, tf ) ] } }\n",
        "  \"\"\"\n",
        "  full_ind = {}\n",
        "  for i in posts.index:\n",
        "    id = posts['Id'][i]\n",
        "    words = posts['Words'][i]\n",
        "    seen = [] #pour ne traiter qu'une fois un mot par document\n",
        "    for word in words:\n",
        "      if word not in full_ind:\n",
        "        seen.append(word)\n",
        "        tf = words.count(word) / len(words)\n",
        "        full_ind[word] = {'df': 1, 'inv_ind': [(id, tf)]}\n",
        "      elif word not in seen :\n",
        "        seen.append(word)\n",
        "        tf = words.count(word) / len(words)\n",
        "        full_ind[word]['df'] += 1\n",
        "        full_ind[word]['inv_ind'].append((id,tf))\n",
        "  return full_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC_maxqT81a2"
      },
      "outputs": [],
      "source": [
        "# full_index = create_inverted_index(posts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEQI8e_P81a2"
      },
      "outputs": [],
      "source": [
        "# Save and Load your Index(es) in Pickle format\n",
        "def save_index(savepath, inverted_index):\n",
        "    \"\"\"Saves the index given as parameter to a `pickle` file\"\"\"\n",
        "    with open(savepath, \"wb\") as file:\n",
        "        pickle.dump(inverted_index, file)\n",
        "\n",
        "\n",
        "def load_index(savepath):\n",
        "    \"\"\"Load the inverted index saved as a `pickle` file\"\"\"\n",
        "    with open(savepath, \"rb\") as file:\n",
        "        loaded_dict = pickle.load(file)\n",
        "    # Access the loaded dictionary\n",
        "    return loaded_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxQRy-oM81a2"
      },
      "outputs": [],
      "source": [
        "full_index = load_index(INVINDEX_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH9PMxRm81a3"
      },
      "source": [
        "# Search Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4VEu31W81a3"
      },
      "source": [
        "## Naive Search and Improvements\n",
        "\n",
        "La fonction à appeler est:\n",
        "```python\n",
        "search_naive(query: str, inverted_index: dict, top: int =5)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aICdKu9g81a3"
      },
      "outputs": [],
      "source": [
        "# Naive search\n",
        "def word_in_index(word: str, word_list_index: list)->pd.Series:\n",
        "  \"\"\"\n",
        "    Implement the word_in_index function \n",
        "    Inputs : a word (str) & a list of words\n",
        "    Output : pandas series of 1 if the word is in the list, else 0\n",
        "  \"\"\"\n",
        "  if word_list_index == []:\n",
        "    return pd.Series(dtype='float64')\n",
        "  df = pd.DataFrame(word_list_index)\n",
        "  df[\"New Word\"] = [word for _ in range(len(word_list_index))]\n",
        "  df[\"Comparison\"] = (df[0] == df[\"New Word\"])\n",
        "  return pd.Series(df[\"Comparison\"])\n",
        "\n",
        "\n",
        "def count_common_words(query: str, word_serie: pd.Series)->pd.Series:\n",
        "  \"\"\"\n",
        "  Implement the function which run through a pandas series and count the number of word in common\n",
        "  Use extract_words method, apply method with word_in_index function\n",
        "  Inputs : the query (str) & pandas series of strings\n",
        "  Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
        "  \"\"\"\n",
        "  query_items = extract_words(query)\n",
        "  return sum(word_in_index(q_word, word_serie) for q_word in query_items)\n",
        "\n",
        "\n",
        "def rank_top_query(query:str, df:pd.DataFrame, top: int = 5)->list:\n",
        "  \"\"\"  \"\"\"\n",
        "  ranking = []\n",
        "  for line in range(df.shape[0]):\n",
        "    post_id = df['Id'][line]\n",
        "    word_ser = df['Words'][line]\n",
        "    nb_comm_words = sum(count_common_words(query, word_ser))\n",
        "    ranking.append([nb_comm_words, post_id])\n",
        "  ranking.sort(reverse=True)\n",
        "  return ranking[0:top]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljhzMc8Z81a3"
      },
      "outputs": [],
      "source": [
        "# Naive but using the inverted index\n",
        "def search_naive(query: str, inverted_index: dict, top: int=5):\n",
        "    query_items = extract_words(query)\n",
        "    ranking = dict()\n",
        "    for word in query_items:\n",
        "        if word in inverted_index:\n",
        "            posting_list = inverted_index[word][\"inv_ind\"]\n",
        "            for post_id, tf in posting_list:\n",
        "                if post_id in ranking:\n",
        "                    ranking[post_id] += tf\n",
        "                else:\n",
        "                    ranking[post_id] = tf\n",
        "        else:\n",
        "            continue\n",
        "    ranking = sorted(ranking.items(), key=lambda item: item[1])\n",
        "    return ranking[0:top]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnlK6vuq81a3"
      },
      "source": [
        "## Boolean Search\n",
        "\n",
        "La fonction à appeler est :\n",
        "```python \n",
        "search_boolean(query: str, inverted_index: dict, booleanOperator: set)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfF0Kksc81a3"
      },
      "outputs": [],
      "source": [
        "# Boolean Search\n",
        "inverted_index_simple = {}\n",
        "for word in full_index:\n",
        "  l=[]\n",
        "  tuple_list = full_index[word]['inv_ind']\n",
        "  for elt in tuple_list:\n",
        "    (doc_id,_)=elt #elt = (a,b)\n",
        "    l.append(doc_id)\n",
        "  inverted_index_simple[word]=l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2iRJvE_481a4"
      },
      "outputs": [],
      "source": [
        "# la requête est sous la formenormale conjonctive A1 OR A2 OR A3 OR A4...\n",
        "# transforme la requête en booléen\n",
        "def transformation_query_to_boolean(query: str):\n",
        "    boolean_query=[]\n",
        "    for token in query.split():\n",
        "        boolean_query.append(token)\n",
        "        boolean_query.append('AND')\n",
        "    boolean_query.pop()\n",
        "    return boolean_query\n",
        "\n",
        "\n",
        "BooleanOperator = {\"AND\", \"OR\", \"NOT\"}\n",
        "\n",
        "def transformation_query_to_postfixe(query: str):\n",
        "    b = BooleanExpression(query)\n",
        "    return b.postfix_tokens\n",
        "\n",
        "# merge deux posting lists selon l'opérateur\n",
        "def merge_and_postings_list(posting_term1: list, posting_term2: list)->list:\n",
        "    result=[]\n",
        "    n = len(posting_term1)\n",
        "    m = len(posting_term2)\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while i < n and j <m:\n",
        "        if posting_term1[i] == posting_term2[j]:\n",
        "            result.append(posting_term1[i])\n",
        "            i = i+1\n",
        "            j = j+1\n",
        "        else:\n",
        "            if posting_term1[i] < posting_term2[j]:\n",
        "                i = i+1\n",
        "            else:\n",
        "                j=j+1\n",
        "    return result\n",
        "\n",
        "def merge_or_postings_list(posting_term1: list, posting_term2: list)->list:\n",
        "    result=[]\n",
        "    n = len(posting_term1)\n",
        "    m = len(posting_term2)\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while i < n and j <m:\n",
        "        if posting_term1[i] == posting_term2[j]:\n",
        "            result.append(posting_term1[i])\n",
        "            i = i+1\n",
        "            j = j+1\n",
        "        else:\n",
        "            if posting_term1[i] < posting_term2[j]:\n",
        "                result.append(posting_term1[i])\n",
        "                i = i+1\n",
        "            else:\n",
        "                result.append(posting_term2[j])\n",
        "                j=j+1\n",
        "    if i <n:\n",
        "        result = result + posting_term1[i:]\n",
        "    if j <m:\n",
        "        result = result + posting_term2[j:]\n",
        "    return result\n",
        "\n",
        "def merge_and_not_postings_list(posting_term1: list, posting_term2: list)->list:\n",
        "    result=[]\n",
        "    n = len(posting_term1)\n",
        "    m = len(posting_term2)\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while i < n and j <m:\n",
        "        if posting_term1[i] == posting_term2[j]:\n",
        "            i = i+1\n",
        "            j = j+1\n",
        "        else:\n",
        "            if posting_term1[i] < posting_term2[j]:\n",
        "                result.append(posting_term1[i])\n",
        "                i = i+1\n",
        "            else:\n",
        "                j=j+1\n",
        "    return result\n",
        "\n",
        "# généralise le merge selon l'opérateur\n",
        "def boolean_operator_processing_with_inverted_index(BoolOperator: str, posting_term1: list, posting_term2: list)->list:\n",
        "    result=[]\n",
        "    if BoolOperator == \"AND\":\n",
        "        result.append(merge_and_postings_list(posting_term1,posting_term2))\n",
        "    elif BoolOperator==\"OR\" :\n",
        "        result.append(merge_or_postings_list(posting_term1,posting_term2))\n",
        "    elif BoolOperator == \"NOT\":\n",
        "        result.append(merge_and_not_postings_list(posting_term1,posting_term2))\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSt3SpX881a4"
      },
      "outputs": [],
      "source": [
        "def search_boolean(query: str, inverted_index_simple: dict, booleanOperator=BooleanOperator):\n",
        "    evaluation_stack = []\n",
        "    # transformer query en liste de mots\n",
        "    query = extract_words(query)\n",
        "\n",
        "    for term in query:\n",
        "        if term.upper() not in booleanOperator:\n",
        "          evaluation_stack.append(inverted_index_simple[term])#on rajoute la posting list du dernier terme\n",
        "        else:\n",
        "            if term.upper() == \"NOT\":\n",
        "              operande= evaluation_stack.pop()\n",
        "              eval_prop = boolean_operator_processing_with_inverted_index(term.upper(), evaluation_stack.pop(),operande)\n",
        "              evaluation_stack.append(eval_prop[0])\n",
        "            else:\n",
        "              operator = term.upper()\n",
        "              eval_prop =  boolean_operator_processing_with_inverted_index(operator, evaluation_stack.pop(),evaluation_stack.pop())\n",
        "              evaluation_stack.append(eval_prop[0])\n",
        "    return  evaluation_stack.pop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgsBkGHe81a4"
      },
      "source": [
        "## Probabilstic Search (OBM25)\n",
        "\n",
        "La fonction à appeler est :\n",
        "```python \n",
        "search_OBM25(query: str, inverted_index: dict, simple_index: pd.DataFrame, top: int)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYQm8fwE81a4"
      },
      "outputs": [],
      "source": [
        "# Probabilistic Search Okapi BM25\n",
        "\n",
        "def search_OBM25(query: str, inverted_index: dict =full_index,\n",
        "                 simple_index: pd.DataFrame =clean_posts, top: int =5):\n",
        "  #constantes \n",
        "  k1 = 1.2\n",
        "  k3 = 1000\n",
        "  b = 0.75\n",
        "  m = np.mean(simple_index['len']) #longueur moyenne des docs, à trouver\n",
        "  #traitement de la query\n",
        "  query_ind = {}\n",
        "  query_tmt = nltk.word_tokenize(query)\n",
        "\n",
        "  for i in range(len(query_tmt)) : \n",
        "    query_tmt[i] = lemmatizer.lemmatize(query_tmt[i])\n",
        "  for word in query_tmt:\n",
        "    tf = query_tmt.count(word)/len(query_tmt)\n",
        "    query_ind[word] = tf\n",
        "  \n",
        "  N = len(posts)\n",
        "  #CORE on va faire sum(a*b*c) sur les termes pour chaque doc\n",
        "  \n",
        "  RSV = {}\n",
        "\n",
        "  for word in query_ind.keys():\n",
        "    if word in inverted_index:\n",
        "      df_j = inverted_index[word]['df']\n",
        "      \n",
        "      tuple_list = inverted_index[word]['inv_ind']\n",
        "      tf_j_q = query_ind[word]\n",
        "      a3 = math.log((N-df_j+0.5)/df_j+0.5)\n",
        "      a2 = (k3 + 1 ) * tf_j_q / ( k3 + tf_j_q)\n",
        "      for tuple_elt in tuple_list : \n",
        "        (doc_id , tf_j_d) = tuple_elt\n",
        "        L = simple_index.loc[simple_index['Id'] == doc_id].iloc[0]['len']\n",
        "        a1 = (k1 + 1) * tf_j_d / (k1 * ((1-b) + b * L/m) + tf_j_d)\n",
        "        if not(doc_id in RSV) :\n",
        "          RSV[doc_id] = a1 * a2 *a3\n",
        "        else :\n",
        "          RSV[doc_id] += a1 *a2 * a3\n",
        "\n",
        "  RSV = sorted(RSV.items(), key=lambda x: x[1], reverse=True)\n",
        "  return RSV[0:top]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9UkrTEw81a4"
      },
      "source": [
        "## MIB\n",
        "\n",
        "La fonction à appeler est :\n",
        "```python\n",
        "search_MIB(quey: str, inverted_index: dict, top: int)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM6fYPgV81a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def search_MIB(query: str, inverted_index: dict =full_index, data=posts, top: int =5):\n",
        "  N = len(data) #nombre des posts\n",
        "  tokens = nltk.word_tokenize(query)\n",
        "  Docs_id = dict()\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
        "    if tokens[i] in inverted_index:\n",
        "      for j in range(len(inverted_index[tokens[i]]['inv_ind'])):\n",
        "        if inverted_index[tokens[i]]['inv_ind'][j][0] not in Docs_id:\n",
        "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] = np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
        "        else:\n",
        "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] += np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
        "  sort_orders = sorted(Docs_id.items(), key=lambda x: x[1], reverse=True)\n",
        "  return sort_orders[0:top]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAX0bnGo81a5"
      },
      "source": [
        "## TF-IDF\n",
        "\n",
        "La fonction à appeler est :\n",
        "```python\n",
        "vectorizer_search(query : str, data=posts, vectors=vectors, vectorizer=vectorizer)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GesNK5na81a5"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(posts.Cleaned_post.values)\n",
        "vectors = vectorizer.transform(posts.Cleaned_post.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4ExTv1I81a5"
      },
      "outputs": [],
      "source": [
        "def clean_query(query: str)->str:\n",
        "    return query.lower()\n",
        "\n",
        "def vectorize_query(query : str, vectorizer=vectorizer):\n",
        "    \"\"\"Vectorizes the query\n",
        "    Args:\n",
        "        query (str): query string\n",
        "        vectorizer (optional): Defaults to vectorizer.\n",
        "\n",
        "    Returns:\n",
        "        query vectorized\n",
        "    \"\"\"\n",
        "    query_vectorized = vectorizer.transform([clean_query(query)])\n",
        "    return query_vectorized\n",
        "\n",
        "\n",
        "def search_tfidf(query : str,\n",
        "                 data=posts,\n",
        "                 vectors=vectors,\n",
        "                 vectorizer=vectorizer) -> list:\n",
        "    #renvoit une liste de rankings (doc_id, score)\n",
        "    query_vectorized = vectorize_query(query, vectorizer) \n",
        "    results =  vectors @ query_vectorized.transpose()\n",
        "    res_dict = {}\n",
        "    doc_id_array = results.tocoo().row\n",
        "    for i in range(len(doc_id_array)):\n",
        "      res_dict[data.iloc[doc_id_array[i]]['Id']] = results.data[i]\n",
        "    return sorted(res_dict.items(), key=lambda x: x[1], reverse=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xuhihjze81a5"
      },
      "source": [
        "## Semantic Similarity\n",
        "\n",
        "La fonction à appeler est :\n",
        "```python\n",
        "search_semantic(query: str, data=posts, embeddings=embeddings, top: int =10)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywtzmCfx81a5"
      },
      "outputs": [],
      "source": [
        "sentence_transformer_model = 'multi-qa-mpnet-base-dot-v1'\n",
        "MODEL_ST = SentenceTransformer(sentence_transformer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YluSDIZh81a5"
      },
      "outputs": [],
      "source": [
        "# Prend du temps à éxecuter (~15mins)\n",
        "# embeddings = MODEL_ST.encode(posts.cleaned_body.values, normalize_embeddings=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTVvluAp81a5"
      },
      "outputs": [],
      "source": [
        "# Save/Load the embeddings into pickle format\n",
        "def save_embedding(savepath, embeddings):\n",
        "    with open(savepath, 'wb') as file:\n",
        "        pickle.dump(embeddings, file)\n",
        "\n",
        "def load_embedding(savepath):\n",
        "    with open(savepath, \"rb\") as file:\n",
        "        return pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKUFo9Mg81a9"
      },
      "outputs": [],
      "source": [
        "embeddings = load_embedding(EMBEDDING_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge1E0J-481a9"
      },
      "outputs": [],
      "source": [
        "def encode_query(query : str) ->  np.ndarray:\n",
        "    encoded_query = MODEL_ST.encode(query)\n",
        "    return encoded_query\n",
        "\n",
        "\n",
        "def similarity(query: str, embeddings=embeddings):\n",
        "    query_emb = encode_query(query)\n",
        "    similarity_matrix = util.dot_score(query_emb, embeddings)[0].cpu().tolist()\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def ordre_en_fonction_similarité(matrix_similarity)->list[tuple]:\n",
        "    scores = [(i, matrix_similarity[i]) for i in range(len(matrix_similarity))]\n",
        "    ordre = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    return ordre \n",
        "\n",
        "\n",
        "def search_semantic(query: str, data=posts, embeddings=embeddings, top: int =10)->list[tuple]:\n",
        "    sim_mat = similarity(query, embeddings)\n",
        "    sorted_indexes = ordre_en_fonction_similarité(sim_mat)\n",
        "    max_score = sorted_indexes[0][1]\n",
        "    closest_posts = [(data.Id.iloc[j], score / max_score) for (j, score) in sorted_indexes]\n",
        "    return closest_posts[0:top]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaiacRzA81a9"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvpU1tge81a9"
      },
      "outputs": [],
      "source": [
        "# Vectorize document using TF-IDF\n",
        "vectorizer_lda = TfidfVectorizer()\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = vectorizer_lda.fit_transform(posts.Cleaned_post.values)\n",
        "\n",
        "num_topics = 200\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
        "lda_model.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcdXY84c81a9"
      },
      "outputs": [],
      "source": [
        "def topic_find(text):\n",
        "  new_document_vectorized = vectorizer_lda.transform([text])\n",
        "  topic_probabilities = lda_model.transform(new_document_vectorized)\n",
        "  dominant_topic = topic_probabilities.argmax()\n",
        "  return dominant_topic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWEabpee81a-"
      },
      "outputs": [],
      "source": [
        "posts['Topic'] = posts['Cleaned_post'].fillna('').apply(topic_find)\n",
        "posts['Words'] = posts['Cleaned_post'].fillna('').apply(remove_tags).apply(extract_words).apply(filter_stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knbaDk9A81a-"
      },
      "outputs": [],
      "source": [
        "def get_topic_query(text, vectorizer=vectorizer_lda, lda_model=lda_model) -> int:\n",
        "  new_document_vectorized = vectorizer.transform([text])\n",
        "  topic_probabilities = lda_model.transform(new_document_vectorized)\n",
        "  dominant_topic = topic_probabilities.argmax()\n",
        "  return dominant_topic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wFsnhLZ81a-"
      },
      "outputs": [],
      "source": [
        "query = 'draw neural networks'\n",
        "topic_query = get_topic_query(query)\n",
        "topic_k = posts.loc[posts['Topic'] == topic_query]\n",
        "\n",
        "full_ind_k = create_inverted_index(topic_k)\n",
        "\n",
        "test_cluster_without = search_OBM25(query, full_index)\n",
        "test_cluster_with = search_OBM25(query, full_ind_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DodShWOy81a-"
      },
      "outputs": [],
      "source": [
        "def search(query):\n",
        "    # TODO\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYKhEPWy81a-"
      },
      "source": [
        "# MERGED SEARCH METHOD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iA7IcFA81a-"
      },
      "outputs": [],
      "source": [
        "def nlp_search_algorithm(query,\n",
        "                         ponderation = 1/6*np.ones((6)),\n",
        "                         data=posts,\n",
        "                         inverted_index=full_index,\n",
        "                         simple_index=clean_posts,\n",
        "                         # topic_documents=topic_documents,\n",
        "                         vectors=vectors,\n",
        "                         vectorizer=vectorizer,\n",
        "                         vectorizer_lda=vectorizer_lda,\n",
        "                         lda_model=lda_model,\n",
        "                         embeddings=embeddings,\n",
        "                         top_n=10\n",
        "                         )->list:\n",
        "    #Pondération\n",
        "    coefs = ponderation\n",
        "    \n",
        "    #Récupération de tous les rankings listes pour la query\n",
        "    rankings = []\n",
        "    tip_top = min(top_n, data.shape[0])\n",
        "\n",
        "    r_boolean = search_boolean(query, inverted_index_simple )\n",
        "    rankings.append(r_boolean)\n",
        "\n",
        "    #pour traiter le plus de documents pertinents possibles\n",
        "    tip_top = max(tip_top, len(r_boolean)) \n",
        "    \n",
        "    r_naive = search_naive(query, inverted_index, tip_top) \n",
        "    rankings.append(r_naive)\n",
        "\n",
        "    r_obm25 = search_OBM25(query, inverted_index , simple_index , tip_top)\n",
        "    rankings.append(r_obm25)\n",
        "\n",
        "    r_MIB = search_MIB(query, inverted_index, data, tip_top)\n",
        "    rankings.append(r_MIB)\n",
        "\n",
        "    r_tfidf = search_tfidf(query, data, vectors, vectorizer)\n",
        "    rankings.append(r_tfidf)\n",
        "\n",
        "    r_semantic = search_semantic(query, data, embeddings, tip_top)\n",
        "    rankings.append(r_semantic)\n",
        "\n",
        "    ranking_dict = {}\n",
        "\n",
        "    #On traite tous les modèles sauf le booléen\n",
        "    for i in range(1, len(rankings)) : \n",
        "      r_i = rankings[i]\n",
        "      for j in range(len(r_i)):\n",
        "        (doc_id, score_j) = r_i[j]\n",
        "        if doc_id not in ranking_dict : \n",
        "          ranking_dict[doc_id] = coefs[i] * score_j / log( j + 2 )\n",
        "        else : \n",
        "          ranking_dict[doc_id] += coefs[i] * score_j / log( j + 2 )\n",
        "\n",
        "    #On ajoute booléen, qui va pénaliser les docs où les termes sont absents\n",
        "    N = len(rankings[0])\n",
        "    for j in range( N ):\n",
        "        if doc_id not in ranking_dict : \n",
        "          ranking_dict[doc_id] = coefs[0] / N\n",
        "        else : \n",
        "          ranking_dict[doc_id] += coefs[0] / N\n",
        "\n",
        "    matching_posts = sorted(ranking_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    return matching_posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck93Puwx81a_"
      },
      "source": [
        "## Metadonnées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iaq001081a_"
      },
      "outputs": [],
      "source": [
        "\"\"\" A partir d'une liste de résultats d'une fonction de search, on utilise\n",
        "le score de chaque posts pour affiner la pertinence, en faisant compter un poids c_score\"\"\"\n",
        "\n",
        "def metadata_score( ranking_list : list[tuple], c_score:float) -> list[tuple] :\n",
        "  meta_list = [] #list[tuple] [(doc_id, score)]\n",
        "  for couple in ranking_list:\n",
        "    meta_list.append((couple[0],posts.loc['Id' == id]['Score'].value))\n",
        "  #On ordonne la liste des rankings par score\n",
        "  meta_ranking_list = sorted(meta_list, key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "  #On utilise la structure de dictionnaire pour accéder facilement aux doc_id\n",
        "  new_ranking = {}\n",
        "  for cpl in ranking_list:\n",
        "    new_ranking[cpl[0]] = cpl[1] #new_ranking[doc_id] = score_init\n",
        "  #On normalise le poids grâce à l'élément de plus haut score\n",
        "  c2 = c_score * new_ranking[meta_ranking_list[0][0]] \n",
        "  N = len(meta_ranking_list)\n",
        "  \n",
        "  for j in range(N) : \n",
        "    new_ranking[meta_ranking_list[j][0]] += c2 *(N-j)/N \n",
        "  \n",
        "  return sorted(new_ranking.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Recherche par contexte"
      ],
      "metadata": {
        "id": "JD2WeCTPBif6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Notation: on note docs(t_i) la liste des documents jugés pertinents suite à la recherche du term_i par search engine.\n",
        "#But : En utilisant le réseau de neurones word2vec skip gram, trouver les mots liés à t_i dans le contexte donné, et renvoyer docs(t_i) inter docs(t_j) au lieu de simplement docs(t_i)"
      ],
      "metadata": {
        "id": "ODkKFhqFBGAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def softmax(x):\n",
        "\t\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "\te_x = np.exp(x - np.max(x))\n",
        "\treturn e_x / e_x.sum()\n",
        "\n",
        "class word2vec(object):\n",
        "\tdef __init__(self):\n",
        "\t\tself.N = 10\n",
        "\t\tself.X_train = []\n",
        "\t\tself.y_train = []\n",
        "\t\tself.window_size = 2\n",
        "\t\tself.alpha = 0.001\n",
        "\t\tself.words = []\n",
        "\t\tself.word_index = {}\n",
        "\n",
        "\tdef initialize(self,V,data):\n",
        "\t\tself.V = V\n",
        "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "\t\t\n",
        "\t\tself.words = data\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tself.word_index[data[i]] = i\n",
        "\n",
        "\t\n",
        "\tdef feed_forward(self,X):\n",
        "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "\t\tself.u = np.dot(self.W1.T,self.h)\n",
        "\t\t#print(self.u)\n",
        "\t\tself.y = softmax(self.u)\n",
        "\t\treturn self.y\n",
        "\t\t\n",
        "\tdef backpropagate(self,x,t):\n",
        "\t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
        "\t\t# e.shape is V x 1\n",
        "\t\tdLdW1 = np.dot(self.h,e.T)\n",
        "\t\tX = np.array(x).reshape(self.V,1)\n",
        "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "\t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
        "\t\tself.W = self.W - self.alpha*dLdW\n",
        "\t\t\n",
        "\tdef train(self,epochs):\n",
        "\t\tfor x in range(1,epochs):\t\n",
        "\t\t\tself.loss = 0\n",
        "\t\t\tfor j in range(len(self.X_train)):\n",
        "\t\t\t\tself.feed_forward(self.X_train[j])\n",
        "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
        "\t\t\t\tC = 0\n",
        "\t\t\t\tfor m in range(self.V):\n",
        "\t\t\t\t\tif(self.y_train[j][m]):\n",
        "\t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
        "\t\t\t\t\t\tC += 1\n",
        "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
        "\t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
        "\t\t\t\n",
        "\tdef predict(self,word,number_of_predictions):\n",
        "\t\tif word in self.words:\n",
        "\t\t\tindex = self.word_index[word]\n",
        "\t\t\tX = [0 for i in range(self.V)]\n",
        "\t\t\tX[index] = 1\n",
        "\t\t\tprediction = self.feed_forward(X)\n",
        "\t\t\toutput = {}\n",
        "\t\t\tfor i in range(self.V):\n",
        "\t\t\t\toutput[prediction[i][0]] = i\n",
        "\t\t\t\n",
        "\t\t\ttop_context_words = []\n",
        "\t\t\tfor k in sorted(output,reverse=True):\n",
        "\t\t\t\ttop_context_words.append(self.words[output[k]])\n",
        "\t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
        "\t\t\t\t\tbreak\n",
        "\t\n",
        "\t\t\treturn top_context_words\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Word not found in dictionary\")\n"
      ],
      "metadata": {
        "id": "JmdkGQASB3qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(corpus):\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttraining_data = []\n",
        "\tsentences = corpus.split(\".\")\n",
        "\tfor i in range(len(sentences)):\n",
        "\t\tsentences[i] = sentences[i].strip()\n",
        "\t\tsentence = sentences[i].split()\n",
        "\t\tx = [word.strip(string.punctuation) for word in sentence\n",
        "\t\t\t\t\t\t\t\t\tif word not in stop_words]\n",
        "\t\tx = [word.lower() for word in x]\n",
        "\t\ttraining_data.append(x)\n",
        "\treturn training_data\n",
        "\t\n",
        "\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "\tdata = {}\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor word in sentence:\n",
        "\t\t\tif word not in data:\n",
        "\t\t\t\tdata[word] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tdata[word] += 1\n",
        "\tV = len(data)\n",
        "\tdata = sorted(list(data.keys()))\n",
        "\tvocab = {}\n",
        "\tfor i in range(len(data)):\n",
        "\t\tvocab[data[i]] = i\n",
        "\t\n",
        "\t#for i in range(len(words)):\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor i in range(len(sentence)):\n",
        "\t\t\tcenter_word = [0 for x in range(V)]\n",
        "\t\t\tcenter_word[vocab[sentence[i]]] = 1\n",
        "\t\t\tcontext = [0 for x in range(V)]\n",
        "\t\t\t\n",
        "\t\t\tfor j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "\t\t\t\tif i!=j and j>=0 and j<len(sentence):\n",
        "\t\t\t\t\tcontext[vocab[sentence[j]]] += 1\n",
        "\t\t\tw2v.X_train.append(center_word)\n",
        "\t\t\tw2v.y_train.append(context)\n",
        "\tw2v.initialize(V,data)\n",
        "\n",
        "\treturn w2v.X_train,w2v.y_train\n"
      ],
      "metadata": {
        "id": "1rs01y41BgRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#But ici: bien l'entrainer\n",
        "#récuper 1 posts sur 10 et lui donner à manger\n",
        "#mode d'emploi: \n",
        "#recup1_10() permet de nourrire le corpus"
      ],
      "metadata": {
        "id": "LEIGBBbSB-UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pre-processing pour le petit réseau de neurones"
      ],
      "metadata": {
        "id": "dQHnwIkqEAPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "bodies=posts['Body']\n",
        "corpus=''\n",
        "def recup1_10():\n",
        "\n",
        "  res=''#futur nourriture pour la petit réseau de neurones\n",
        "  c=np.random.randint(0,10)\n",
        "  for text in bodies:\n",
        "    if c==0:\n",
        "      res = text + ' '\n",
        "  corpus = res"
      ],
      "metadata": {
        "id": "IG_FhyanCFUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)\n",
        "\n",
        "#w2v.predict(\"around\",1)\n"
      ],
      "metadata": {
        "id": "NGhbX3R-B05G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implémentation du mot_contexte dans le search_engine"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ew-xWFBiRX3s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indice(element, liste):\n",
        "    try:\n",
        "        indice = liste.index(element)\n",
        "        return indice\n",
        "    except ValueError:\n",
        "        return None\n",
        "def merge_rank(l1,l2): #l1=classement des docs pertinents pour le terme de la requête, l2 pour le mot mot contexte\n",
        "  final_list=[]\n",
        "  for i1 in range(len(l1)):\n",
        "    if l1[i1] not in l2:\n",
        "      final_list.append((l1[i1],0.8*(i1+1)+0.2*5))#on le pénalise de ne pas être dans la liste pertinents liés au mot contexte\n",
        "    elif l1[i1] in l2:\n",
        "      i2=get_indice(l1[i1],l2)\n",
        "      final_list.append((0.8*(i1+1)+0.2*(i2+1)))#les rank apparents sont bien flottants mais pas grave\n",
        "  for i2 in range(len(l2)):\n",
        "    if l2[i2] not in l1:\n",
        "      final_list.append((l2[i2],0.8*10+0.2*i2))\n",
        "  sorted(final_list,key=lambda x:x[1])#on trie par rang\n",
        "  return final_list#liste des docs_id\n",
        "\n",
        "#But=faire fonction qui permet d'utiliser aussi le mot_context\n",
        "def final_search_engine(query):\n",
        "  for term in query:\n",
        "    mot_context=w2v.predict('term')[0]\n",
        "    l1=search_engine(term)\n",
        "    l2=search_engine(mot_context)\n",
        "    merge_rank(l1,l2)#mais il reste la question de comment on marge les final_list entre mots de la query\n",
        "                    "
      ],
      "metadata": {
        "id": "dmB7JFMWPwuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFr2QrKc81a_"
      },
      "source": [
        "## Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbDh6dua81a_"
      },
      "outputs": [],
      "source": [
        "def rank_search(results, top=5):\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_results[0:top]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si9orzOB81a_"
      },
      "source": [
        "## Visualising Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXcOBxMO81a_"
      },
      "outputs": [],
      "source": [
        "def visualize_output(query, results):\n",
        "    print(\"La requête :\")\n",
        "    print(query)\n",
        "    print(\"Résultats :\")\n",
        "    for post_id, score in results:\n",
        "      print(posts[\"Title\"][posts[\"Id\"]==post_id])\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he4w1PFX81a_"
      },
      "outputs": [],
      "source": [
        "print('Without clustering')\n",
        "visualize_output(query, test_cluster_without)\n",
        "\n",
        "print('With clustering')\n",
        "visualize_output(query, test_cluster_with)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU1F73Q_81bA"
      },
      "source": [
        "## Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQO4rXFs81bA"
      },
      "outputs": [],
      "source": [
        "def make_query(natural_query):\n",
        "    # TODO\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDHrX9Cz81bA"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KllRxjI81bA"
      },
      "outputs": [],
      "source": [
        "# Pas sûr de garder cette partie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsZvNGKx81bA"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLJNCs4g81bA"
      },
      "outputs": [],
      "source": [
        "# Read Relevancy CSV\n",
        "# /!\\ changer le filepath\n",
        "df_relevancy = pd.read_excel(os.path.join(DATA_PATH, \"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\"))\n",
        "df_relevancy = df_relevancy.fillna(0)\n",
        "df_relevancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c98eZYH181bA"
      },
      "outputs": [],
      "source": [
        "test_queries = {1:'Query 1 : mesure performance for multiclassification model',\n",
        "                2:'Query 2 : draw neural network',\n",
        "                3:'Query 3 : neural network layers',\n",
        "                4:'Query 4 : how sklearn working',\n",
        "                5:'Query 5 : treat categorical data',\n",
        "                'Query 1 : mesure performance for multiclassification model': 1,\n",
        "                'Query 2 : draw neural network': 2,\n",
        "                'Query 3 : neural network layers': 3,\n",
        "                'Query 4 : how sklearn working': 4,\n",
        "                'Query 5 : treat categorical data': 5}\n",
        "\n",
        "def calc_dcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
        "  dcg = 0\n",
        "  for k in range(rank):\n",
        "    id = query_results[k]\n",
        "    score = df_relevancy[test_queries[query_number]][df_relevancy[\"PostId\"]==id].iloc[0]/ (log(k+2)/log(2))\n",
        "    dcg +=  score \n",
        "  return dcg\n",
        "\n",
        "\n",
        "def calc_dcg_ideal(rank: int =5, query_number: int =1)->float:\n",
        "  dcg_ideal = 0\n",
        "  perfect_ranking = sorted(list(df_relevancy[test_queries[query_number]]), reverse=True)\n",
        "  for k in range(rank):\n",
        "    dcg_ideal += perfect_ranking[k] / log(k+2, 2)\n",
        "  return dcg_ideal\n",
        "\n",
        "\n",
        "def calculate_ndcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
        "  return calc_dcg(query_results, rank, query_number) / calc_dcg_ideal(rank, query_number)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fdj2yL881bA"
      },
      "source": [
        "TESTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXx_IpYk81bB"
      },
      "outputs": [],
      "source": [
        "subset_docs = set(df_relevancy[\"PostId\"])\n",
        "subset_posts = clean_posts[clean_posts['Id'].isin(subset_docs)]\n",
        "subset_posts['Cleaned_post'] = subset_posts['Body'].fillna('').apply(clean_post)\n",
        "subset_invind = create_inverted_index(subset_posts)\n",
        "subset_vectorizer = TfidfVectorizer()\n",
        "subset_vectorizer.fit(subset_posts.Cleaned_post.values)\n",
        "subset_vectors = subset_vectorizer.transform(subset_posts.Cleaned_post.values)\n",
        "subset_embeddings = MODEL_ST.encode(subset_posts.Cleaned_post.values, normalize_embeddings=True)\n",
        "\n",
        "# First tests 'a la mano'\n",
        "print(calc_dcg(sorted(list(subset_docs), reverse=True)))\n",
        "print(calc_dcg_ideal())\n",
        "print(calculate_ndcg(sorted(list(subset_docs), reverse=True)))\n",
        "# ideal ranking found by hand for the first test query\n",
        "ideal_ranking = [13490, 15989, 6107, 12321, 22, 14899, 5706, 15135, 12851, 694, 9302, 9443]\n",
        "print(calculate_ndcg(ideal_ranking))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5b76-0E81bB"
      },
      "outputs": [],
      "source": [
        "# Test for our previous search models\n",
        "query_1 = 'mesure performance for multiclassification model'\n",
        "query_2 = 'draw neural network'\n",
        "naive_result = search_naive(query_2, subset_invind, 10)\n",
        "bm25_result = search_OBM25(query_2, subset_invind, subset_posts, top=10)\n",
        "mib_result = search_MIB(query_2, subset_invind, 10)\n",
        "tfidf_result = search_tfidf(query_2, subset_posts, subset_vectors, subset_vectorizer)\n",
        "semantic_result = search_semantic(query_2, subset_posts, subset_embeddings, 10)\n",
        "\n",
        "naive_ranking = [r[0] for r in naive_result]\n",
        "bm25_ranking  = [r[0] for r in bm25_result]\n",
        "mib_ranking   = [r[0] for r in mib_result]\n",
        "tfidf_ranking = [r[0] for r in tfidf_result]\n",
        "semantic_ranking = [r[0] for r in semantic_result]\n",
        "\n",
        "print(calculate_ndcg(naive_ranking, query_number=2))\n",
        "print(calculate_ndcg(bm25_ranking, query_number=2))\n",
        "print(calculate_ndcg(mib_ranking, query_number=2))\n",
        "print(calculate_ndcg(tfidf_ranking, query_number=2))\n",
        "print(calculate_ndcg(semantic_ranking, query_number=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Yksr2Sg81bB"
      },
      "outputs": [],
      "source": [
        "visualize_output('machine learning basic tutorial best', nlp_search_algorithm('machine learning basic tutorial best'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZWv938M81bB"
      },
      "source": [
        "## Find a better ponderation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNFv8kT-81bB"
      },
      "outputs": [],
      "source": [
        "def rand_pond(nb_models=6):\n",
        "  pond = [np.random.random() for _ in range(nb_models)]\n",
        "  s = sum(pond)\n",
        "  return [i/s for i in pond]\n",
        "\n",
        "def get_imax(liste):\n",
        "  i_max = 0\n",
        "  for j in range(1, len(liste)):\n",
        "    if liste[i_max] < liste[j]:\n",
        "      i_max = j\n",
        "  return i_max\n",
        "\n",
        "def find_ponderation(query_number=2, nb_reps=10, nb_models=6):\n",
        "  to_test = [rand_pond(nb_models) for _ in range(nb_reps)]\n",
        "  ndcg_list = []\n",
        "  for pond in to_test:\n",
        "    query = test_queries[query_number]\n",
        "    results = nlp_search_algorithm(query, ponderation=pond, data=subset_posts, inverted_index=subset_invind,\n",
        "                     simple_index=clean_posts,\n",
        "                     vectors=subset_vectors,\n",
        "                     vectorizer=subset_vectorizer,\n",
        "                     embeddings=subset_embeddings)\n",
        "    ndcg_list.append(calculate_ndcg(results, query_number))\n",
        "  i_max = get_imax(ndcg_list)\n",
        "  return to_test[i_max]\n",
        "\n",
        "\n",
        "find_ponderation()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}