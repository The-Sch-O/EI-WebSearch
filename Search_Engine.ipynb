{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install py7zr\n",
    "! pip install tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import py7zr\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from tt import BooleanExpression\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('all')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On google colab use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MAIN_PATH = '/content/drive/MyDrive/TP Centrale'\n",
    "DATA_PATH = '/content/drive/MyDrive/TP Centrale/data'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in VS Code use this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = ''\n",
    "DATA_PATH = '/data'\n",
    "INVINDEX_PATH = \"inverted_index.pickle\"\n",
    "\n",
    "STOPS = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    if not os.path.isdir(MAIN_PATH):\n",
    "        os.mkdir(MAIN_PATH)\n",
    "    if not os.path.isdir(MAIN_PATH):\n",
    "        os.mkdir(DATA_PATH)\n",
    "    archive = py7zr.SevenZipFile(os.path.join(MAIN_PATH, 'datascience.stackexchange.com.7z'), mode='r')\n",
    "    archive.extractall(path=os.path.join(MAIN_PATH, 'data'))\n",
    "    archive.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def index_data():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text:str)->list:\n",
    "  \"\"\"Transforms a given text into a list of tokens\"\"\"\n",
    "  tokens = text.lower()\n",
    "  tokens = nltk.tokenize.word_tokenize(tokens)\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i].rstrip(\".!?,;:\\(\\)\\\"\\'\")\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def remove_tags(text: str) -> str:\n",
    "    \"\"\"Remove the HTML tags from a given text\"\"\"\n",
    "    cleaned_text = re.sub(r'<.*?>', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespaces\n",
    "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing whitespaces\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def filter_stop_words(words:list[str]) -> list[str]:\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    if word not in STOPS:\n",
    "        new_words.append(word)\n",
    "  return new_words\n",
    "\n",
    "\n",
    "def inverted_index_data():\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts = posts[['Id','Body']]\n",
    "clean_posts['Words'] = clean_posts['Body'].fillna('').apply(remove_tags).apply(extract_words).apply(filter_stop_words)\n",
    "\n",
    "def create_inverted_index(posts:pd.DataFrame)->dict:\n",
    "  \"\"\"\n",
    "  On suppose que les posts ne sont pas pré-traités. \n",
    "  On va renvoyer un index inversé complet et un index des TF\n",
    "  full_ind = {key : {'df' : int , 'inv_ind' : [ (id, tf ) ] } }\n",
    "  \"\"\"\n",
    "  full_ind = {}\n",
    "  for i in range(len(posts['Id'])):\n",
    "    words = posts['Words'][i]; id = posts['Id'][i]\n",
    "    seen = [] #pour ne traiter qu'une fois un mot par document\n",
    "    for word in words:\n",
    "      if word not in full_ind:\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word] = {'df': 1, 'inv_ind': [(id, tf)]}\n",
    "      elif word not in seen :\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word]['df'] += 1\n",
    "        full_ind[word]['inv_ind'].append((id,tf))\n",
    "  return full_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = create_inverted_index(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load your Index(es) in Pickle format\n",
    "def save_index(savepath, inverted_index):\n",
    "    \"\"\"Saves the index given as parameter to a `pickle` file\"\"\"\n",
    "    with open(savepath, \"wb\") as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "\n",
    "\n",
    "def load_index(savepath):\n",
    "    \"\"\"Load the inverted index saved as a `pickle` file\"\"\"\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    # Access the loaded dictionary\n",
    "    return loaded_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Search and Improvements\n",
    "\n",
    "La fonction à appeler est:\n",
    "```python\n",
    "opti_naive_search(query: str, inv_index: dict, top: int =5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive search\n",
    "def word_in_index(word: str, word_list_index: list)->pd.Series:\n",
    "  \"\"\"\n",
    "    Implement the word_in_index function \n",
    "    Inputs : a word (str) & a list of words\n",
    "    Output : pandas series of 1 if the word is in the list, else 0\n",
    "  \"\"\"\n",
    "  if word_list_index == []:\n",
    "    return pd.Series(dtype='float64')\n",
    "  df = pd.DataFrame(word_list_index)\n",
    "  df[\"New Word\"] = [word for _ in range(len(word_list_index))]\n",
    "  df[\"Comparison\"] = (df[0] == df[\"New Word\"])\n",
    "  return pd.Series(df[\"Comparison\"])\n",
    "\n",
    "\n",
    "def count_common_words(query: str, word_serie: pd.Series)->pd.Series:\n",
    "  \"\"\"\n",
    "  Implement the function which run through a pandas series and count the number of word in common\n",
    "  Use extract_words method, apply method with word_in_index function\n",
    "  Inputs : the query (str) & pandas series of strings\n",
    "  Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
    "  \"\"\"\n",
    "  query_items = extract_words(query)\n",
    "  return sum(word_in_index(q_word, word_serie) for q_word in query_items)\n",
    "\n",
    "\n",
    "def rank_top_query(query:str, df:pd.DataFrame, top: int = 5)->list:\n",
    "  \"\"\"  \"\"\"\n",
    "  ranking = []\n",
    "  for line in range(df.shape[0]):\n",
    "    post_id = df['Id'][line]\n",
    "    word_ser = df['Words'][line]\n",
    "    nb_comm_words = sum(count_common_words(query, word_ser))\n",
    "    ranking.append([nb_comm_words, post_id])\n",
    "  ranking.sort(reverse=True)\n",
    "  return ranking[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive but using the inverted index\n",
    "def opti_naive_search(query: str, inv_index: dict, top: int=5):\n",
    "    query_items = extract_words(query)\n",
    "    ranking = dict()\n",
    "    for word in query_items:\n",
    "        posting_list = inv_index[word][\"inv_ind\"]\n",
    "        for post_id, tf in posting_list:\n",
    "            if post_id in ranking:\n",
    "                ranking[post_id] += tf\n",
    "            else:\n",
    "                ranking[post_id] = tf\n",
    "    ranking = sorted(ranking.items(), key=lambda item: item[1])\n",
    "    return ranking[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python \n",
    "processing_boolean_query_with_inverted_index(booleanOperator: set, query: str, inverted_index: dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Search\n",
    "inv_index_simple = {}\n",
    "for word in full_index:\n",
    "  l=[]\n",
    "  tuple_list = full_index[word]['inv_ind']\n",
    "  for elt in tuple_list:\n",
    "    (doc_id,_)=elt #elt = (a,b)\n",
    "    l.append(doc_id)\n",
    "  inv_index_simple[word]=l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la requête est sous la formenormale conjonctive A1 OR A2 OR A3 OR A4...\n",
    "# transforme la requête en booléen\n",
    "def transformation_query_to_boolean(query: str):\n",
    "    boolean_query=[]\n",
    "    for token in query.split():\n",
    "        boolean_query.append(token)\n",
    "        boolean_query.append('AND')\n",
    "    boolean_query.pop()\n",
    "    return boolean_query\n",
    "\n",
    "\n",
    "BooleanOperator = {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "def transformation_query_to_postfixe(query: str):\n",
    "    b = BooleanExpression(query)\n",
    "    return b.postfix_tokens\n",
    "\n",
    "# merge deux posting lists selon l'opérateur\n",
    "def merge_and_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "def merge_or_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                result.append(posting_term2[j])\n",
    "                j=j+1\n",
    "    if i <n:\n",
    "        result = result + posting_term1[i:]\n",
    "    if j <m:\n",
    "        result = result + posting_term2[j:]\n",
    "    return result\n",
    "\n",
    "def merge_and_not_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "# généralise le merge selon l'opérateur\n",
    "def boolean_operator_processing_with_inverted_index(BoolOperator: str, posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    if BoolOperator == \"AND\":\n",
    "        result.append(merge_and_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator==\"OR\" :\n",
    "        result.append(merge_or_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator == \"NOT\":\n",
    "        result.append(merge_and_not_postings_list(posting_term1,posting_term2))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_boolean_query_with_inverted_index(booleanOperator, query, inverted_index):\n",
    "    evaluation_stack = []\n",
    "    # transformer query en liste de mots\n",
    "    query = extract_words(query)\n",
    "\n",
    "    for term in query:\n",
    "        if term.upper() not in booleanOperator:\n",
    "          evaluation_stack.append(inverted_index[term.upper()])#on rajoute la posting list du dernier terme\n",
    "        else:\n",
    "            if term.upper() == \"NOT\":\n",
    "              operande= evaluation_stack.pop()\n",
    "              eval_prop = boolean_operator_processing_with_inverted_index(term.upper(), evaluation_stack.pop(),operande)\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "            else:\n",
    "              operator = term.upper()\n",
    "              eval_prop =  boolean_operator_processing_with_inverted_index(operator, evaluation_stack.pop(),evaluation_stack.pop())\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "    return  evaluation_stack.pop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilstic Search (OBM25)\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python \n",
    "probabilistic_search_OBM25(query: str)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilistic Search Okapi BM25\n",
    "clean_posts['len'] = clean_posts['Words'].apply(len) #On a besoin de cette donnée en accès rapide\n",
    "\n",
    "def probabilistic_search_OBM25(query: str, inverted_index: dict =full_index, simple_index=clean_posts, top: int =50):\n",
    "  #constantes \n",
    "  k1 = 1.2\n",
    "  k3 = 1000\n",
    "  b = 0.75\n",
    "  m = np.mean(simple_index['len']) #longueur moyenne des docs, à trouver\n",
    "  #traitement de la query\n",
    "  query_ind = {}\n",
    "  query_tmt = nltk.word_tokenize(query)\n",
    "\n",
    "  for i in range(len(query_tmt)) : \n",
    "    query_tmt[i] = lemmatizer.lemmatize(query_tmt[i])\n",
    "  for word in query_tmt:\n",
    "    tf = query_tmt.count(word)/len(query_tmt)\n",
    "    query_ind[word] = tf\n",
    "  \n",
    "  N = len(posts)\n",
    "  #CORE on va faire sum(a*b*c) sur les termes pour chaque doc\n",
    "  \n",
    "  RSV = {}\n",
    "\n",
    "  for word in query_ind.keys():\n",
    "    if word in inverted_index:\n",
    "      df_j = inverted_index[word]['df']\n",
    "      \n",
    "      tuple_list = inverted_index[word]['inv_ind']\n",
    "      tf_j_q = query_ind[word]\n",
    "      a3 = math.log((N-df_j+0.5)/df_j+0.5)\n",
    "      a2 = (k3 + 1 ) * tf_j_q / ( k3 + tf_j_q)\n",
    "      for tuple_elt in tuple_list : \n",
    "        (doc_id , tf_j_d) = tuple_elt\n",
    "        L = simple_index.loc[simple_index['Id'] == doc_id].iloc[0]['len']\n",
    "        a1 = (k1 + 1) * tf_j_d / ( k1((1-b) + b * L/m) + tf_j_d)\n",
    "        if not(doc_id in RSV) :\n",
    "          RSV[doc_id] = a1 * a2 *a3\n",
    "        else :\n",
    "          RSV[doc_id] += a1 *a2 * a3\n",
    "\n",
    "  RSV = sorted(RSV.items(), key=lambda x: x[1], reverse=True)\n",
    "  return RSV[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIB\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "probabilistic_search_MIB(quey, inverted_index, top)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 75727 #nombre des posts\n",
    "\n",
    "def probabilistic_search_MIB(query: str, inverted_index: dict =full_index, top: int =5):\n",
    "  tokens = nltk.word_tokenize(query)\n",
    "  Docs_id = dict()\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "    if tokens[i] in inverted_index:\n",
    "      for j in range(len(inverted_index[tokens[i]]['inv_ind'])):\n",
    "        if inverted_index[tokens[i]]['inv_ind'][j][0] not in Docs_id:\n",
    "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] = np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
    "        else:\n",
    "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] += np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
    "  sort_orders = sorted(Docs_id.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sort_orders[0:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_search(results, top=5):\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query(natural_query):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûr de garder cette partie"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Relevancy CSV\n",
    "# /!\\ changer le filepath\n",
    "df_relevancy = pd.read_excel(os.path.join(DATA_PATH, \"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\"))\n",
    "df_relevancy = df_relevancy.fillna(0)\n",
    "df_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {1:'Query 1 : mesure performance for multiclassification model',\n",
    "                2:'Query 2 : draw neural network',\n",
    "                3:'Query 3 : neural network layers',\n",
    "                4:'Query 4 : how sklearn working',\n",
    "                5:'Query 5 : treat categorical data',\n",
    "                'Query 1 : mesure performance for multiclassification model': 1,\n",
    "                'Query 2 : draw neural network': 2,\n",
    "                'Query 3 : neural network layers': 3,\n",
    "                'Query 4 : how sklearn working': 4,\n",
    "                'Query 5 : treat categorical data': 5}\n",
    "\n",
    "def calc_dcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  dcg = 0\n",
    "  for k in range(rank):\n",
    "    id = query_results[k]\n",
    "    score = df_relevancy[test_queries[query_number]][df_relevancy[\"PostId\"]==id].iloc[0]/ (log(k+2)/log(2))\n",
    "    dcg +=  score \n",
    "  return dcg\n",
    "\n",
    "\n",
    "def calc_dcg_ideal(rank: int =5, query_number: int =1)->float:\n",
    "  dcg_ideal = 0\n",
    "  perfect_ranking = sorted(list(df_relevancy[test_queries[query_number]]), reverse=True)\n",
    "  for k in range(rank):\n",
    "    dcg_ideal += perfect_ranking[k] / log(k+2, 2)\n",
    "  return dcg_ideal\n",
    "\n",
    "\n",
    "def calculate_ndcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  return calc_dcg(query_results, rank, query_number) / calc_dcg_ideal(rank, query_number)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_relevancy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theon\\Documents\\GitHub\\EI-WebSearch\\Search_Engine.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/theon/Documents/GitHub/EI-WebSearch/Search_Engine.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m subset_docs \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(df_relevancy[\u001b[39m\"\u001b[39m\u001b[39mPostId\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theon/Documents/GitHub/EI-WebSearch/Search_Engine.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m subset_posts \u001b[39m=\u001b[39m clean_posts[clean_posts[\u001b[39m'\u001b[39m\u001b[39mId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(subset_docs)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theon/Documents/GitHub/EI-WebSearch/Search_Engine.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m subset_invind \u001b[39m=\u001b[39m create_inverted_index(subset_posts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_relevancy' is not defined"
     ]
    }
   ],
   "source": [
    "subset_docs = set(df_relevancy[\"PostId\"])\n",
    "subset_posts = clean_posts[clean_posts['Id'].isin(subset_docs)]\n",
    "subset_invind = create_inverted_index(subset_posts)\n",
    "\n",
    "print(calc_dcg(sorted(list(subset_docs), reverse=True)))\n",
    "print(calc_dcg_ideal())\n",
    "print(calculate_ndcg(sorted(list(subset_docs), reverse=True)))\n",
    "# ideal ranking found by hand for the first test query\n",
    "ideal_ranking = [13490, 15989, 6107, 12321, 22, 14899, 5706, 15135, 12851, 694, 9302, 9443]\n",
    "print(calculate_ndcg(ideal_ranking))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
