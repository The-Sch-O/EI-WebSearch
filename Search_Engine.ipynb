{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install py7zr\n",
    "! pip install ttable\n",
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import py7zr\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from tt import BooleanExpression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('all')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On google colab use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MAIN_PATH = '/content/drive/MyDrive/TP Centrale'\n",
    "DATA_PATH = '/content/drive/MyDrive/TP Centrale/data'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in VS Code use this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN_PATH = ''\n",
    "# DATA_PATH = '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVINDEX_PATH = os.path.join(DATA_PATH, \"inverted_index.pickle\")\n",
    "EMBEDDING_PATH = os.path.join(DATA_PATH, \"embeddings.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    if not os.path.isdir(MAIN_PATH):\n",
    "        os.mkdir(MAIN_PATH)\n",
    "    if not os.path.isdir(MAIN_PATH):\n",
    "        os.mkdir(DATA_PATH)\n",
    "    archive = py7zr.SevenZipFile(os.path.join(MAIN_PATH, 'datascience.stackexchange.com.7z'), mode='r')\n",
    "    archive.extractall(path=os.path.join(MAIN_PATH, 'data'))\n",
    "    archive.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def index_data():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text:str)->list:\n",
    "  \"\"\"Transforms a given text into a list of tokens\"\"\"\n",
    "  tokens = text.lower()\n",
    "  tokens = nltk.tokenize.word_tokenize(tokens)\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i].rstrip(\".!?,;:\\(\\)\\\"\\'\")\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def remove_tags(text: str) -> str:\n",
    "    \"\"\"Remove the HTML tags from a given text\"\"\"\n",
    "    cleaned_text = re.sub(r'<.*?>', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespaces\n",
    "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing whitespaces\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def filter_stop_words(words:list[str]) -> list[str]:\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    if word not in stops:\n",
    "        new_words.append(word)\n",
    "  return new_words\n",
    "\n",
    "\n",
    "def clean_post(text:str):\n",
    "  text = remove_tags(text)\n",
    "  text = text.lower()\n",
    "  res = nltk.word_tokenize(text)\n",
    "  clean = ''\n",
    "  for i in range(len(res)):\n",
    "    res[i] = lemmatizer.lemmatize(res[i])\n",
    "    if res[i] not in stops and res[i] not in string.punctuation:\n",
    "      clean += str(res[i]) + ' '\n",
    "  return clean\n",
    "\n",
    "\n",
    "def inverted_index_data():\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts = posts[['Id','Body']]\n",
    "clean_posts['Words'] = clean_posts['Body'].fillna('').apply(remove_tags).apply(extract_words).apply(filter_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(posts:pd.DataFrame)->dict:\n",
    "  \"\"\"\n",
    "  On suppose que les posts sont pré-traités. \n",
    "  On va renvoyer un index inversé complet et un index des TF\n",
    "  full_ind = {key : {'df' : int , 'inv_ind' : [ (id, tf ) ] } }\n",
    "  \"\"\"\n",
    "  full_ind = {}\n",
    "  for i in posts.index:\n",
    "    id = posts['Id'][i]\n",
    "    words = posts['Words'][i]\n",
    "    seen = [] #pour ne traiter qu'une fois un mot par document\n",
    "    for word in words:\n",
    "      if word not in full_ind:\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word] = {'df': 1, 'inv_ind': [(id, tf)]}\n",
    "      elif word not in seen :\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word]['df'] += 1\n",
    "        full_ind[word]['inv_ind'].append((id,tf))\n",
    "  return full_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_index = create_inverted_index(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load your Index(es) in Pickle format\n",
    "def save_index(savepath, inverted_index):\n",
    "    \"\"\"Saves the index given as parameter to a `pickle` file\"\"\"\n",
    "    with open(savepath, \"wb\") as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "\n",
    "\n",
    "def load_index(savepath):\n",
    "    \"\"\"Load the inverted index saved as a `pickle` file\"\"\"\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    # Access the loaded dictionary\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = load_index(INVINDEX_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Search and Improvements\n",
    "\n",
    "La fonction à appeler est:\n",
    "```python\n",
    "search_naive(query: str, inverted_index: dict, top: int =5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive search\n",
    "def word_in_index(word: str, word_list_index: list)->pd.Series:\n",
    "  \"\"\"\n",
    "    Implement the word_in_index function \n",
    "    Inputs : a word (str) & a list of words\n",
    "    Output : pandas series of 1 if the word is in the list, else 0\n",
    "  \"\"\"\n",
    "  if word_list_index == []:\n",
    "    return pd.Series(dtype='float64')\n",
    "  df = pd.DataFrame(word_list_index)\n",
    "  df[\"New Word\"] = [word for _ in range(len(word_list_index))]\n",
    "  df[\"Comparison\"] = (df[0] == df[\"New Word\"])\n",
    "  return pd.Series(df[\"Comparison\"])\n",
    "\n",
    "\n",
    "def count_common_words(query: str, word_serie: pd.Series)->pd.Series:\n",
    "  \"\"\"\n",
    "  Implement the function which run through a pandas series and count the number of word in common\n",
    "  Use extract_words method, apply method with word_in_index function\n",
    "  Inputs : the query (str) & pandas series of strings\n",
    "  Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
    "  \"\"\"\n",
    "  query_items = extract_words(query)\n",
    "  return sum(word_in_index(q_word, word_serie) for q_word in query_items)\n",
    "\n",
    "\n",
    "def rank_top_query(query:str, df:pd.DataFrame, top: int = 5)->list:\n",
    "  \"\"\"  \"\"\"\n",
    "  ranking = []\n",
    "  for line in range(df.shape[0]):\n",
    "    post_id = df['Id'][line]\n",
    "    word_ser = df['Words'][line]\n",
    "    nb_comm_words = sum(count_common_words(query, word_ser))\n",
    "    ranking.append([nb_comm_words, post_id])\n",
    "  ranking.sort(reverse=True)\n",
    "  return ranking[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive but using the inverted index\n",
    "def search_naive(query: str, inverted_index: dict, top: int=5):\n",
    "    query_items = extract_words(query)\n",
    "    ranking = dict()\n",
    "    for word in query_items:\n",
    "        if word in inverted_index:\n",
    "            posting_list = inverted_index[word][\"inv_ind\"]\n",
    "            for post_id, tf in posting_list:\n",
    "                if post_id in ranking:\n",
    "                    ranking[post_id] += tf\n",
    "                else:\n",
    "                    ranking[post_id] = tf\n",
    "        else:\n",
    "            continue\n",
    "    ranking = sorted(ranking.items(), key=lambda item: item[1])\n",
    "    return ranking[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python \n",
    "search_boolean(query: str, inverted_index: dict, booleanOperator: set)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Search\n",
    "inverted_index_simple = {}\n",
    "for word in full_index:\n",
    "  l=[]\n",
    "  tuple_list = full_index[word]['inv_ind']\n",
    "  for elt in tuple_list:\n",
    "    (doc_id,_)=elt #elt = (a,b)\n",
    "    l.append(doc_id)\n",
    "  inverted_index_simple[word]=l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la requête est sous la formenormale conjonctive A1 OR A2 OR A3 OR A4...\n",
    "# transforme la requête en booléen\n",
    "def transformation_query_to_boolean(query: str):\n",
    "    boolean_query=[]\n",
    "    for token in query.split():\n",
    "        boolean_query.append(token)\n",
    "        boolean_query.append('AND')\n",
    "    boolean_query.pop()\n",
    "    return boolean_query\n",
    "\n",
    "\n",
    "BooleanOperator = {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "def transformation_query_to_postfixe(query: str):\n",
    "    b = BooleanExpression(query)\n",
    "    return b.postfix_tokens\n",
    "\n",
    "# merge deux posting lists selon l'opérateur\n",
    "def merge_and_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "def merge_or_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                result.append(posting_term2[j])\n",
    "                j=j+1\n",
    "    if i <n:\n",
    "        result = result + posting_term1[i:]\n",
    "    if j <m:\n",
    "        result = result + posting_term2[j:]\n",
    "    return result\n",
    "\n",
    "def merge_and_not_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "# généralise le merge selon l'opérateur\n",
    "def boolean_operator_processing_with_inverted_index(BoolOperator: str, posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    if BoolOperator == \"AND\":\n",
    "        result.append(merge_and_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator==\"OR\" :\n",
    "        result.append(merge_or_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator == \"NOT\":\n",
    "        result.append(merge_and_not_postings_list(posting_term1,posting_term2))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_boolean(query: str, inverted_index_simple: dict, booleanOperator=BooleanOperator):\n",
    "    evaluation_stack = []\n",
    "    # transformer query en liste de mots\n",
    "    query = extract_words(query)\n",
    "\n",
    "    for term in query:\n",
    "        if term.upper() not in booleanOperator:\n",
    "          evaluation_stack.append(inverted_index_simple[term.upper()])#on rajoute la posting list du dernier terme\n",
    "        else:\n",
    "            if term.upper() == \"NOT\":\n",
    "              operande= evaluation_stack.pop()\n",
    "              eval_prop = boolean_operator_processing_with_inverted_index(term.upper(), evaluation_stack.pop(),operande)\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "            else:\n",
    "              operator = term.upper()\n",
    "              eval_prop =  boolean_operator_processing_with_inverted_index(operator, evaluation_stack.pop(),evaluation_stack.pop())\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "    return  evaluation_stack.pop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilstic Search (OBM25)\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python \n",
    "search_OBM25(query: str, inverted_index: dict, simple_index: pd.DataFrame, top: int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilistic Search Okapi BM25\n",
    "clean_posts['len'] = clean_posts['Words'].apply(len) #On a besoin de cette donnée en accès rapide\n",
    "\n",
    "def search_OBM25(query: str, inverted_index: dict =full_index,\n",
    "                 simple_index: pd.DataFrame =clean_posts, top: int =5):\n",
    "  #constantes \n",
    "  k1 = 1.2\n",
    "  k3 = 1000\n",
    "  b = 0.75\n",
    "  m = np.mean(simple_index['len']) #longueur moyenne des docs, à trouver\n",
    "  #traitement de la query\n",
    "  query_ind = {}\n",
    "  query_tmt = nltk.word_tokenize(query)\n",
    "\n",
    "  for i in range(len(query_tmt)) : \n",
    "    query_tmt[i] = lemmatizer.lemmatize(query_tmt[i])\n",
    "  for word in query_tmt:\n",
    "    tf = query_tmt.count(word)/len(query_tmt)\n",
    "    query_ind[word] = tf\n",
    "  \n",
    "  N = len(posts)\n",
    "  #CORE on va faire sum(a*b*c) sur les termes pour chaque doc\n",
    "  \n",
    "  RSV = {}\n",
    "\n",
    "  for word in query_ind.keys():\n",
    "    if word in inverted_index:\n",
    "      df_j = inverted_index[word]['df']\n",
    "      \n",
    "      tuple_list = inverted_index[word]['inv_ind']\n",
    "      tf_j_q = query_ind[word]\n",
    "      a3 = math.log((N-df_j+0.5)/df_j+0.5)\n",
    "      a2 = (k3 + 1 ) * tf_j_q / ( k3 + tf_j_q)\n",
    "      for tuple_elt in tuple_list : \n",
    "        (doc_id , tf_j_d) = tuple_elt\n",
    "        L = simple_index.loc[simple_index['Id'] == doc_id].iloc[0]['len']\n",
    "        a1 = (k1 + 1) * tf_j_d / (k1 * ((1-b) + b * L/m) + tf_j_d)\n",
    "        if not(doc_id in RSV) :\n",
    "          RSV[doc_id] = a1 * a2 *a3\n",
    "        else :\n",
    "          RSV[doc_id] += a1 *a2 * a3\n",
    "\n",
    "  RSV = sorted(RSV.items(), key=lambda x: x[1], reverse=True)\n",
    "  return RSV[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIB\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "search_MIB(quey: str, inverted_index: dict, top: int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def search_MIB(query: str, inverted_index: dict =full_index, data=posts, top: int =5):\n",
    "  N = len(data) #nombre des posts\n",
    "  tokens = nltk.word_tokenize(query)\n",
    "  Docs_id = dict()\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "    if tokens[i] in inverted_index:\n",
    "      for j in range(len(inverted_index[tokens[i]]['inv_ind'])):\n",
    "        if inverted_index[tokens[i]]['inv_ind'][j][0] not in Docs_id:\n",
    "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] = np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
    "        else:\n",
    "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] += np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
    "  sort_orders = sorted(Docs_id.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sort_orders[0:top]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "vectorizer_search(query : str, data=posts, vectors=vectors, vectorizer=vectorizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['Cleaned_post'] = posts['Body'].fillna('').apply(clean_post)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(posts.Cleaned_post.values)\n",
    "vectors = vectorizer.transform(posts.Cleaned_post.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_post(text:str):\n",
    "  text = remove_tags(text)\n",
    "  text = text.lower()\n",
    "  res = nltk.word_tokenize(text)\n",
    "  clean = ''\n",
    "  for i in range(len(res)):\n",
    "    res[i] = lemmatizer.lemmatize(res[i])\n",
    "    if res[i] not in stops and res[i] not in string.punctuation:\n",
    "      clean += str(res[i]) + ' '\n",
    "  return clean[:-1]\n",
    "\n",
    "\n",
    "def clean_query(query: str)->str:\n",
    "    return query.lower()\n",
    "\n",
    "def vectorize_query(query : str, vectorizer=vectorizer):\n",
    "    \"\"\"Vectorizes the query\n",
    "    Args:\n",
    "        query (str): query string\n",
    "        vectorizer (optional): Defaults to vectorizer.\n",
    "\n",
    "    Returns:\n",
    "        query vectorized\n",
    "    \"\"\"\n",
    "    query_vectorized = vectorizer.transform([clean_query(query)])\n",
    "    return query_vectorized\n",
    "\n",
    "\n",
    "def search_tfidf(query : str,\n",
    "                 data=posts,\n",
    "                 vectors=vectors,\n",
    "                 vectorizer=vectorizer) -> list:\n",
    "    #renvoit une liste de rankings (doc_id, score)\n",
    "    query_vectorized = vectorize_query(query, vectorizer) \n",
    "    results =  vectors @ query_vectorized.transpose()\n",
    "    res_dict = {}\n",
    "    doc_id_array = results.tocoo().row\n",
    "    for i in range(len(doc_id_array)):\n",
    "      res_dict[ data.iloc[ doc_id_array[i]]['Id'] ] = results.data[i]\n",
    "    return sorted(res_dict.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "search_semantic(query: str, data=posts, embeddings=embeddings, top: int =10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = 'multi-qa-mpnet-base-dot-v1'\n",
    "MODEL_ST = SentenceTransformer(sentence_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prend du temps à éxecuter (~15mins)\n",
    "# embeddings = MODEL_ST.encode(posts.cleaned_body.values, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load the embeddings into pickle format\n",
    "def save_embedding(savepath, embeddings):\n",
    "    with open(savepath, 'wb') as file:\n",
    "        pickle.dump(embeddings, file)\n",
    "\n",
    "def load_embedding(savepath):\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_embedding(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query : str) ->  np.ndarray:\n",
    "    encoded_query = MODEL_ST.encode(query)\n",
    "    return encoded_query\n",
    "\n",
    "\n",
    "def similarity(query: str, embeddings=embeddings):\n",
    "    query_emb = encode_query(query)\n",
    "    similarity_matrix = util.dot_score(query_emb, embeddings)[0].cpu().tolist()\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def ordre_en_fonction_similarité(matrix_similarity)->list[tuple]:\n",
    "    scores = [(i, matrix_similarity[i]) for i in range(len(matrix_similarity))]\n",
    "    ordre = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return ordre \n",
    "\n",
    "\n",
    "def search_semantic(query: str, data=posts, embeddings=embeddings, top: int =10)->list[tuple]:\n",
    "    sim_mat = similarity(query, embeddings)\n",
    "    sorted_indexes = ordre_en_fonction_similarité(sim_mat)\n",
    "    max_score = sorted_indexes[0][1]\n",
    "    closest_posts = [(data.Id.iloc[j], score / max_score) for (j, score) in sorted_indexes]\n",
    "    return closest_posts[0:top]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize document using TF-IDF\n",
    "vectorizer_lda = TfidfVectorizer()\n",
    "\n",
    "# Fit and Transform the documents\n",
    "train_data = vectorizer_lda.fit_transform(posts.Cleaned_post.values)\n",
    "\n",
    "num_topics = 200\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
    "lda_model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_find(text):\n",
    "  new_document_vectorized = vectorizer_lda.transform([text])\n",
    "  topic_probabilities = lda_model.transform(new_document_vectorized)\n",
    "  dominant_topic = topic_probabilities.argmax()\n",
    "  return dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['Topic'] = posts['Cleaned_post'].fillna('').apply(topic_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_query(text, vectorizer=vectorizer_lda, lda_model=lda_model) -> int:\n",
    "  new_document_vectorized = vectorizer.transform([text])\n",
    "  topic_probabilities = lda_model.transform(new_document_vectorized)\n",
    "  dominant_topic = topic_probabilities.argmax()\n",
    "  return dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_query = get_topic_query(query)\n",
    "topic_k = posts.loc[posts['Topic'] == topic_query]\n",
    "\n",
    "full_ind_k = create_inverted_index(topic_k)\n",
    "\n",
    "search_MIB(query, full_ind_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGED SEARCH METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_search_algorithm(query,\n",
    "                         data=posts,\n",
    "                         inverted_index=full_index,\n",
    "                         simple_index=clean_posts,\n",
    "                         topic_documents=topic_documents,\n",
    "                         vectors=vectors,\n",
    "                         vectorizer=vectorizer,\n",
    "                         vectorizer_lda=vectorizer_lda,\n",
    "                         lda_model=lda_model,\n",
    "                         embeddings=embeddings,\n",
    "                         top_n=10\n",
    "                         )->list:\n",
    "    #Pondération\n",
    "    coefs = []\n",
    "    \n",
    "    #Récupération de tous les rankings listes pour la query\n",
    "    rankings = []\n",
    "    tip_top = min(top_n, data.shape[0])\n",
    "\n",
    "    r_boolean = search_boolean(query, inverted_index_simple )\n",
    "    rankings.append(r_boolean)\n",
    "\n",
    "    #pour traiter le plus de documents pertinents possibles\n",
    "    tip_top = max(tip_top, len(r_boolean)) \n",
    "    \n",
    "    r_naive = search_naive(query, inverted_index, tip_top) \n",
    "    rankings.append(r_naive)\n",
    "\n",
    "    r_obm25 = search_OBM25(query, inverted_index , simple_index , tip_top)\n",
    "    rankings.append(r_obm25)\n",
    "\n",
    "    r_MIB = search_MIB(query, inverted_index, tip_top)\n",
    "    rankings.append(r_MIB)\n",
    "\n",
    "    r_tfidf = search_tfidf(query, data, vectors, vectorizer)\n",
    "    rankings.append(r_tfidf)\n",
    "\n",
    "    r_semantic = search_semantic(query, data, embeddings, tip_top)\n",
    "    rankings.append(r_semantic)\n",
    "\n",
    "    ranking_dict = {}\n",
    "\n",
    "    #On traite tous les modèles sauf le booléen\n",
    "    for i in range(1, len(rankings)) : \n",
    "      r_i = rankings[i]\n",
    "      for j in range(len(r_i)):\n",
    "        (doc_id, score_j) = r_i[j]\n",
    "        if doc_id not in ranking_dict : \n",
    "          ranking_dict[doc_id] = coefs[i] * score_j / log( j + 2 )\n",
    "        else : \n",
    "          ranking_dict[doc_id] += coefs[i] * score_j / log( j + 2 )\n",
    "\n",
    "    #On ajoute booléen, qui va pénaliser les docs où les termes sont absents\n",
    "    N = len(rankings[0])\n",
    "    for j in range( N ):\n",
    "        if doc_id not in ranking_dict : \n",
    "          ranking_dict[doc_id] = coefs[0] / N\n",
    "        else : \n",
    "          ranking_dict[doc_id] += coefs[0] / N\n",
    "\n",
    "    matching_posts = sorted(ranking_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return matching_posts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_search(results, top=5):\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query(natural_query):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûr de garder cette partie"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Relevancy CSV\n",
    "# /!\\ changer le filepath\n",
    "df_relevancy = pd.read_excel(os.path.join(DATA_PATH, \"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\"))\n",
    "df_relevancy = df_relevancy.fillna(0)\n",
    "df_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {1:'Query 1 : mesure performance for multiclassification model',\n",
    "                2:'Query 2 : draw neural network',\n",
    "                3:'Query 3 : neural network layers',\n",
    "                4:'Query 4 : how sklearn working',\n",
    "                5:'Query 5 : treat categorical data',\n",
    "                'Query 1 : mesure performance for multiclassification model': 1,\n",
    "                'Query 2 : draw neural network': 2,\n",
    "                'Query 3 : neural network layers': 3,\n",
    "                'Query 4 : how sklearn working': 4,\n",
    "                'Query 5 : treat categorical data': 5}\n",
    "\n",
    "def calc_dcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  dcg = 0\n",
    "  for k in range(rank):\n",
    "    id = query_results[k]\n",
    "    score = df_relevancy[test_queries[query_number]][df_relevancy[\"PostId\"]==id].iloc[0]/ (log(k+2)/log(2))\n",
    "    dcg +=  score \n",
    "  return dcg\n",
    "\n",
    "\n",
    "def calc_dcg_ideal(rank: int =5, query_number: int =1)->float:\n",
    "  dcg_ideal = 0\n",
    "  perfect_ranking = sorted(list(df_relevancy[test_queries[query_number]]), reverse=True)\n",
    "  for k in range(rank):\n",
    "    dcg_ideal += perfect_ranking[k] / log(k+2, 2)\n",
    "  return dcg_ideal\n",
    "\n",
    "\n",
    "def calculate_ndcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  return calc_dcg(query_results, rank, query_number) / calc_dcg_ideal(rank, query_number)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_docs = set(df_relevancy[\"PostId\"])\n",
    "subset_posts = clean_posts[clean_posts['Id'].isin(subset_docs)]\n",
    "subset_posts['Cleaned_post'] = subset_posts['Body'].fillna('').apply(clean_post)\n",
    "subset_invind = create_inverted_index(subset_posts)\n",
    "subset_vectorizer = TfidfVectorizer()\n",
    "subset_vectorizer.fit(subset_posts.Cleaned_post.values)\n",
    "subset_vectors = subset_vectorizer.transform(subset_posts.Cleaned_post.values)\n",
    "subset_embeddings = MODEL_ST.encode(subset_posts.Cleaned_post.values, normalize_embeddings=True)\n",
    "\n",
    "# First tests 'a la mano'\n",
    "print(calc_dcg(sorted(list(subset_docs), reverse=True)))\n",
    "print(calc_dcg_ideal())\n",
    "print(calculate_ndcg(sorted(list(subset_docs), reverse=True)))\n",
    "# ideal ranking found by hand for the first test query\n",
    "ideal_ranking = [13490, 15989, 6107, 12321, 22, 14899, 5706, 15135, 12851, 694, 9302, 9443]\n",
    "print(calculate_ndcg(ideal_ranking))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for our previous search models\n",
    "query_1 = 'mesure performance for multiclassification model'\n",
    "query_2 = 'draw neural network'\n",
    "naive_result = search_naive(query_2, subset_invind, 10)\n",
    "bm25_result = search_OBM25(query_2, subset_invind, subset_posts, top=10)\n",
    "mib_result = search_MIB(query_2, subset_invind, 10)\n",
    "tfidf_result = search_tfidf(query_2, subset_posts, subset_vectors, subset_vectorizer)\n",
    "semantic_result = search_semantic(query_2, subset_posts, subset_embeddings, 10)\n",
    "\n",
    "naive_ranking = [r[0] for r in naive_result]\n",
    "bm25_ranking  = [r[0] for r in bm25_result]\n",
    "mib_ranking   = [r[0] for r in mib_result]\n",
    "tfidf_ranking = [r[0] for r in tfidf_result]\n",
    "semantic_ranking = [r[0] for r in semantic_result]\n",
    "\n",
    "print(calculate_ndcg(naive_ranking, query_number=2))\n",
    "print(calculate_ndcg(bm25_ranking, query_number=2))\n",
    "print(calculate_ndcg(mib_ranking, query_number=2))\n",
    "print(calculate_ndcg(tfidf_ranking, query_number=2))\n",
    "print(calculate_ndcg(semantic_ranking, query_number=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
