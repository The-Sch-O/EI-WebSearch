{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\theon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from tt import BooleanExpression\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# nltk.download()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = \"\"\n",
    "INVINDEXPATH = \"inverted_index.pickle\"\n",
    "\n",
    "STOPS = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_data(filepath=FILEPATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def index_data():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text:str)->list:\n",
    "  \"\"\"Transforms a given text into a list of tokens\"\"\"\n",
    "  tokens = text.lower()\n",
    "  tokens = nltk.tokenize.word_tokenize(tokens)\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i].rstrip(\".!?,;:\\(\\)\\\"\\'\")\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def remove_tags(text: str) -> str:\n",
    "    \"\"\"Remove the HTML tags from a given text\"\"\"\n",
    "    cleaned_text = re.sub(r'<.*?>', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespaces\n",
    "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing whitespaces\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def filter_stop(words:list[str]) -> list[str]:\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    if word not in STOPS:\n",
    "        new_words.append(word)\n",
    "  return new_words\n",
    "\n",
    "\n",
    "def inverted_index_data():\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts = posts[['Id','Body']]\n",
    "clean_posts['Words'] = clean_posts['Body'].fillna('').apply(remove_tags).apply(extract_words)\n",
    "\n",
    "def create_inverted_index(posts:pd.DataFrame)-> dict :\n",
    "#On suppose posts pas traité. \n",
    "#On va renvoyer un index inversé complet et un index des TF\n",
    "# full_ind = {key : {'df' : int , 'inv_ind' : [ (id, tf ) ] } }\n",
    "  full_ind = {}\n",
    "  for i in range(len(posts['Id'])):\n",
    "    words = posts['words'][i]; id = posts['Id'][i]\n",
    "    seen = [] #pour ne traiter qu'une fois un mot par document\n",
    "    for word in words:\n",
    "      if word not in full_ind:\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word] = {'df' : 1, 'inv_ind' : [(id,tf)] }\n",
    "      elif word not in seen :\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word]['df']+=1\n",
    "        full_ind[word]['inv_ind'].append((id,tf)) \n",
    "  return (full_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = create_inverted_index(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load your Index(es) in Pickle format\n",
    "def save_index(savepath, inverted_index):\n",
    "    \"\"\"Saves the index given as parameter to a `pickle` file\"\"\"\n",
    "    with open(savepath, \"wb\") as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "\n",
    "\n",
    "def load_index(savepath):\n",
    "    \"\"\"Load the inverted index saved as a `pickle` file\"\"\"\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    # Access the loaded dictionary\n",
    "    return loaded_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive search\n",
    "def word_in_index(word: str, word_list_index: list)->pd.Series:\n",
    "  \"\"\"\n",
    "    Implement the word_in_index function \n",
    "    Inputs : a word (str) & a list of words\n",
    "    Output : pandas series of 1 if the word is in the list, else 0\n",
    "  \"\"\"\n",
    "  if word_list_index == []:\n",
    "    return pd.Series(dtype='float64')\n",
    "  df = pd.DataFrame(word_list_index)\n",
    "  df[\"New Word\"] = [word for _ in range(len(word_list_index))]\n",
    "  df[\"Comparison\"] = (df[0] == df[\"New Word\"])\n",
    "  return pd.Series(df[\"Comparison\"])\n",
    "\n",
    "\n",
    "def count_common_words(query: str, word_serie: pd.Series)->pd.Series:\n",
    "  \"\"\"\n",
    "  Implement the function which run through a pandas series and count the number of word in common\n",
    "  Use extract_words method, apply method with word_in_index function\n",
    "  Inputs : the query (str) & pandas series of strings\n",
    "  Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
    "  \"\"\"\n",
    "  query_items = extract_words(query)\n",
    "  return sum(word_in_index(q_word, word_serie) for q_word in query_items)\n",
    "\n",
    "\n",
    "def rank_top_query(query:str, df:pd.DataFrame, top: int = 5)->list:\n",
    "  \"\"\"  \"\"\"\n",
    "  ranking = []\n",
    "  for line in range(df.shape[0]):\n",
    "    post_id = df['Id'][line]\n",
    "    word_ser = df['words'][line]\n",
    "    nb_comm_words = sum(count_common_words(query, word_ser))\n",
    "    ranking.append([nb_comm_words, post_id])\n",
    "  ranking.sort(reverse=True)\n",
    "  return ranking[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive but using the inverted index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Search\n",
    "inv_index_simple = {}\n",
    "for word in full_index:\n",
    "  l=[]\n",
    "  tuple_list = full_index[word]['inv_ind']\n",
    "  for elt in tuple_list:\n",
    "    (doc_id,_)=elt #elt = (a,b)\n",
    "    l.append(doc_id)\n",
    "  inv_index_simple[word]=l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the query est sous la formenormale conjonctive A1 OR A2 OR A3 OR A4...\n",
    "#transforme la requête en booléen\n",
    "def transformation_query_to_boolean(query):\n",
    "    boolean_query=[]\n",
    "    for token in query.split():\n",
    "        boolean_query.append(token)\n",
    "        boolean_query.append('AND')\n",
    "    boolean_query.pop()\n",
    "    return boolean_query\n",
    "\n",
    "\n",
    "BooleanOperator = {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "def transformation_query_to_postfixe(query):\n",
    "    b = BooleanExpression(query)\n",
    "    return b.postfix_tokens\n",
    "\n",
    "#merge deux posting lists selon l'opérateur\n",
    "def merge_and_postings_list(posting_term1, posting_term2):\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "def merge_or_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                result.append(posting_term2[j])\n",
    "                j=j+1\n",
    "    if i <n:\n",
    "        result = result + posting_term1[i:]\n",
    "    if j <m:\n",
    "        result = result + posting_term2[j:]\n",
    "    return result\n",
    "\n",
    "def merge_and_not_postings_list(posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "#généralise le merge selon l'opérateur\n",
    "def boolean_operator_processing_with_inverted_index(BoolOperator,posting_term1,posting_term2):\n",
    "    result=[]\n",
    "    if BoolOperator == \"AND\":\n",
    "        result.append(merge_and_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator==\"OR\" :\n",
    "        result.append(merge_or_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator == \"NOT\":\n",
    "        result.append(merge_and_not_postings_list(posting_term1,posting_term2))\n",
    "    return result\n",
    "\n",
    "def processing_boolean_query_with_inverted_index(booleanOperator, query, inverted_index):\n",
    "    evaluation_stack = []\n",
    "    #transformer query en liste de mots\n",
    "    \"\"\"res = word_tokenize(query)\n",
    "    Docs_id = []\n",
    "    for i in range(len(res)):\n",
    "      res[i] = lemmatizer.lemmatize(res[i])\"\"\"\n",
    "    query=extract_words(query)\n",
    "\n",
    "    for term in query:\n",
    "        if term.upper() not in booleanOperator:\n",
    "          \"\"\"print(term.upper())\n",
    "          print(inverted_index[term.upper()])\"\"\"\n",
    "          evaluation_stack.append(inverted_index[term.upper()])#on rajoute la posting list du dernier terme\n",
    "        else:\n",
    "            if term.upper() == \"NOT\":\n",
    "              operande= evaluation_stack.pop()\n",
    "              eval_prop = boolean_operator_processing_with_inverted_index(term.upper(), evaluation_stack.pop(),operande)\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "            else:\n",
    "              operator = term.upper()\n",
    "              eval_prop =  boolean_operator_processing_with_inverted_index(operator, evaluation_stack.pop(),evaluation_stack.pop())\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "    return  evaluation_stack.pop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilstic Search (OBM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilistic Search Okapi BM25\n",
    "clean_posts['len'] = clean_posts['words'].apply(len) #On a besoin de cette donnée en accès rapide\n",
    "\n",
    "def probabilistic_search_OBM25(query):\n",
    "  #constantes \n",
    "  k1 = 1.2\n",
    "  k3 = 1000\n",
    "  b = 0.75\n",
    "  m = np.mean(clean_posts['len']) #longueur moyenne des docs, à trouver\n",
    "  #traitement de la query\n",
    "  query_ind = {}\n",
    "  query_tmt = nltk.word_tokenize(query)\n",
    "\n",
    "  for i in range(len(query_tmt)) : \n",
    "    query_tmt[i] = lemmatizer.lemmatize(query_tmt[i])\n",
    "  for word in query_tmt:\n",
    "    tf = query_tmt.count(word)/len(query_tmt)\n",
    "    query_ind[word] = tf\n",
    "  \n",
    "  N = len(posts)\n",
    "  #CORE on va faire sum(a*b*c) sur les termes pour chaque doc\n",
    "  \n",
    "  RSV = {}\n",
    "\n",
    "  for word in query_ind.keys():\n",
    "    if word in full_index:\n",
    "      df_j = full_index[word]['df']\n",
    "      \n",
    "      tuple_list = full_index[word]['inv_ind']\n",
    "      tf_j_q = query_ind[word]\n",
    "      a3 = math.log((N-df_j+0.5)/df_j+0.5)\n",
    "      a2 = (k3 + 1 ) * tf_j_q / ( k3 + tf_j_q)\n",
    "      for tuple_elt in tuple_list : \n",
    "        (doc_id , tf_j_d) = tuple_elt\n",
    "        L = clean_posts.loc[clean_posts['Id'] == doc_id].iloc[0]['len']\n",
    "        a1 = (k1 + 1 ) * tf_j_d / ( k1((1-b) + b * L/m) + tf_j_d)\n",
    "        if not(doc_id in RSV) :\n",
    "          RSV[doc_id] = a1 * a2 *a3\n",
    "        else :\n",
    "          RSV[doc_id] += a1 *a2 * a3\n",
    "\n",
    "  RSV = sorted(RSV.items(), key=lambda x: x[1], reverse=True)\n",
    "  return RSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_search(results, top=5):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_query(natural_query):\n",
    "    # TODO\n",
    "\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûr de garder cette partie"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Relevancy CSV\n",
    "# /!\\ changer le filepath\n",
    "df_relevancy = pd.read_excel(\"/content/drive/MyDrive/TP Centrale/data/evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")\n",
    "df_relevancy = df_relevancy.fillna(0)\n",
    "df_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_docs = set(df_relevancy[\"PostId\"])\n",
    "subset_posts = clean_posts[clean_posts['Id'].isin(subset_docs)]\n",
    "#print(subset_docs)\n",
    "#print(subset_posts)\n",
    "test_queries = {1:'Query 1 : mesure performance for multiclassification model',\n",
    "                2:'Query 2 : draw neural network',\n",
    "                3:'Query 3 : neural network layers',\n",
    "                4:'Query 4 : how sklearn working',\n",
    "                5:'Query 5 : treat categorical data',\n",
    "                'Query 1 : mesure performance for multiclassification model': 1,\n",
    "                'Query 2 : draw neural network': 2,\n",
    "                'Query 3 : neural network layers': 3,\n",
    "                'Query 4 : how sklearn working': 4,\n",
    "                'Query 5 : treat categorical data': 5}\n",
    "\n",
    "def calc_dcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  dcg = 0\n",
    "  for k in range(rank):\n",
    "    id = query_results[k]\n",
    "    print(\"id : \", id)\n",
    "    truc = df_relevancy[test_queries[query_number]][df_relevancy[\"PostId\"]==id].iloc[0]/ (log(k+2)/log(2))\n",
    "    print(truc)\n",
    "    dcg +=  truc \n",
    "  return dcg\n",
    "\n",
    "\n",
    "def calc_dcg_ideal(rank: int =5, query_number: int =1)->float:\n",
    "  dcg_ideal = 0\n",
    "  perfect_ranking = sorted(list(df_relevancy[test_queries[query_number]]), reverse=True)\n",
    "  for k in range(rank):\n",
    "    dcg_ideal += perfect_ranking[k] / log(k+2, 2)\n",
    "  return dcg_ideal\n",
    "\n",
    "\n",
    "def calculate_ndcg(query_col=\"query\", output_col=\"query_output\")->float:\n",
    "\n",
    "  return\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
