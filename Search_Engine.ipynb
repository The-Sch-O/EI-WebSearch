{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install py7zr\n",
    "! pip install ttable\n",
    "! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import py7zr\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from math import log\n",
    "from tt import BooleanExpression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('all')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On google colab use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "MAIN_PATH = '/content/drive/MyDrive/TP Centrale'\n",
    "DATA_PATH = '/content/drive/MyDrive/TP Centrale/data'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in VS Code use this :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN_PATH = ''\n",
    "# DATA_PATH = '/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INVINDEX_PATH = os.path.join(DATA_PATH, \"inverted_index.pickle\")\n",
    "EMBEDDING_PATH = os.path.join(DATA_PATH, \"embeddings.pkl\")\n",
    "LDAMODEL_PATH = os.path.join(DATA_PATH, \"lda_model.pkl\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    \"\"\" extraire les données issues de StackExchange \"\"\"\n",
    "    if not os.path.isdir(MAIN_PATH):\n",
    "        os.mkdir(MAIN_PATH)\n",
    "    if not os.path.isdir(MAIN_PATH):\n",
    "        os.mkdir(DATA_PATH)\n",
    "    archive = py7zr.SevenZipFile(os.path.join(MAIN_PATH, 'datascience.stackexchange.com.7z'), mode='r')\n",
    "    archive.extractall(path=os.path.join(MAIN_PATH, 'data'))\n",
    "    archive.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text:str)->list:\n",
    "  \"\"\"Transforms a given text into a list of tokens\"\"\"\n",
    "  tokens = text.lower()\n",
    "  tokens = nltk.tokenize.word_tokenize(tokens)\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i].rstrip(\".!?,;:\\(\\)\\\"\\'\")\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def remove_tags(text: str) -> str:\n",
    "    \"\"\"Remove the HTML tags from a given text\"\"\"\n",
    "    cleaned_text = re.sub(r'<.*?>', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Remove extra whitespaces\n",
    "    cleaned_text = cleaned_text.strip()  # Remove leading/trailing whitespaces\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def filter_stop_words(words:list[str]) -> list[str]:\n",
    "  \"\"\" extrait les mots vides dans la liste words \"\"\"\n",
    "  new_words = []\n",
    "  for word in words:\n",
    "    if word not in stops:\n",
    "        new_words.append(word)\n",
    "  return new_words\n",
    "\n",
    "\n",
    "def clean_post(text:str):\n",
    "  \"\"\" applique toutes les fonctions précédentes en une seule \"\"\"\n",
    "  text = remove_tags(text)\n",
    "  text = text.lower()\n",
    "  res = nltk.word_tokenize(text)\n",
    "  clean = ''\n",
    "  for i in range(len(res)):\n",
    "    res[i] = lemmatizer.lemmatize(res[i])\n",
    "    if res[i] not in stops and res[i] not in string.punctuation:\n",
    "      clean += str(res[i]) + ' '\n",
    "  return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_posts = posts[['Id','Body']]\n",
    "clean_posts['Words'] = clean_posts['Body'].fillna('').apply(remove_tags).apply(extract_words).apply(filter_stop_words)\n",
    "clean_posts['len'] = clean_posts['Words'].apply(len) #On a besoin de cette donnée en accès rapide pour OBM25\n",
    "posts['Cleaned_post'] = posts['Body'].fillna('').apply(clean_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(posts:pd.DataFrame)->dict:\n",
    "  \"\"\"\n",
    "  On suppose que les posts sont pré-traités. \n",
    "  On va renvoyer un index inversé complet et un index des TF\n",
    "  full_ind = {key : {'df' : int , 'inv_ind' : [ (id, tf ) ] } }\n",
    "  \"\"\"\n",
    "  full_ind = {}\n",
    "  for i in posts.index:\n",
    "    id = posts['Id'][i]\n",
    "    words = posts['Words'][i]\n",
    "    seen = [] #pour ne traiter qu'une fois un mot par document\n",
    "    for word in words:\n",
    "      if word not in full_ind:\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word] = {'df': 1, 'inv_ind': [(id, tf)]}\n",
    "      elif word not in seen :\n",
    "        seen.append(word)\n",
    "        tf = words.count(word) / len(words)\n",
    "        full_ind[word]['df'] += 1\n",
    "        full_ind[word]['inv_ind'].append((id,tf))\n",
    "  return full_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_index = create_inverted_index(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load your Index(es) in Pickle format\n",
    "def save_index(savepath, inverted_index):\n",
    "    \"\"\"Saves the index given as parameter to a `pickle` file\"\"\"\n",
    "    with open(savepath, \"wb\") as file:\n",
    "        pickle.dump(inverted_index, file)\n",
    "\n",
    "\n",
    "def load_index(savepath):\n",
    "    \"\"\"Load the inverted index saved as a `pickle` file\"\"\"\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "    # Access the loaded dictionary\n",
    "    return loaded_dict\n",
    "\n",
    "# save_index(INVINDEX_PATH, full_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_index = load_index(INVINDEX_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Search and Improvements\n",
    "\n",
    "La fonction à appeler est:\n",
    "```python\n",
    "search_naive(query: str, inverted_index: dict, top: int =5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive search\n",
    "def word_in_index(word: str, word_list_index: list)->pd.Series:\n",
    "  \"\"\"\n",
    "    Implement the word_in_index function \n",
    "    Inputs : a word (str) & a list of words\n",
    "    Output : pandas series of 1 if the word is in the list, else 0\n",
    "  \"\"\"\n",
    "  if word_list_index == []:\n",
    "    return pd.Series(dtype='float64')\n",
    "  df = pd.DataFrame(word_list_index)\n",
    "  df[\"New Word\"] = [word for _ in range(len(word_list_index))]\n",
    "  df[\"Comparison\"] = (df[0] == df[\"New Word\"])\n",
    "  return pd.Series(df[\"Comparison\"])\n",
    "\n",
    "\n",
    "def count_common_words(query: str, word_serie: pd.Series)->pd.Series:\n",
    "  \"\"\"\n",
    "  Implement the function which run through a pandas series and count the number of word in common\n",
    "  Use extract_words method, apply method with word_in_index function\n",
    "  Inputs : the query (str) & pandas series of strings\n",
    "  Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
    "  \"\"\"\n",
    "  query_items = extract_words(query)\n",
    "  return sum(word_in_index(q_word, word_serie) for q_word in query_items)\n",
    "\n",
    "\n",
    "def rank_top_query(query:str, df:pd.DataFrame, top: int = 5)->list:\n",
    "  \"\"\" gets the ranking associated with the naive search model \"\"\"\n",
    "  ranking = []\n",
    "  for line in range(df.shape[0]):\n",
    "    post_id = df['Id'][line]\n",
    "    word_ser = df['Words'][line]\n",
    "    nb_comm_words = sum(count_common_words(query, word_ser))\n",
    "    ranking.append([nb_comm_words, post_id])\n",
    "  ranking.sort(reverse=True)\n",
    "  return ranking[0:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive but using the inverted index\n",
    "def search_naive(query: str, inverted_index: dict, top: int=5):\n",
    "    \"\"\" amelioration of naive search using the inverted index \"\"\"\n",
    "    query_items = extract_words(query)\n",
    "    ranking = dict()\n",
    "    for word in query_items:\n",
    "        if word in inverted_index:\n",
    "            posting_list = inverted_index[word][\"inv_ind\"]\n",
    "            for post_id, tf in posting_list:\n",
    "                if post_id in ranking:\n",
    "                    ranking[post_id] += tf\n",
    "                else:\n",
    "                    ranking[post_id] = tf\n",
    "        else:\n",
    "            continue\n",
    "    ranking = sorted(ranking.items(), key=lambda item: item[1])\n",
    "    return ranking[0:top]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Search\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python \n",
    "search_boolean(query: str, inverted_index: dict, booleanOperator: set)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Search\n",
    "inverted_index_simple = {}\n",
    "for word in full_index:\n",
    "  l=[]\n",
    "  tuple_list = full_index[word]['inv_ind']\n",
    "  for elt in tuple_list:\n",
    "    (doc_id,_)=elt #elt = (a,b)\n",
    "    l.append(doc_id)\n",
    "  inverted_index_simple[word]=l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la requête est sous la formenormale conjonctive A1 OR A2 OR A3 OR A4...\n",
    "# transforme la requête en booléen\n",
    "def transformation_query_to_boolean(query: str):\n",
    "    boolean_query=[]\n",
    "    for token in query.split():\n",
    "        boolean_query.append(token)\n",
    "        boolean_query.append('AND')\n",
    "    boolean_query.pop()\n",
    "    return boolean_query\n",
    "\n",
    "\n",
    "BooleanOperator = {\"AND\", \"OR\", \"NOT\"}\n",
    "\n",
    "def transformation_query_to_postfixe(query: str):\n",
    "    b = BooleanExpression(query)\n",
    "    return b.postfix_tokens\n",
    "\n",
    "# merge deux posting lists selon l'opérateur\n",
    "def merge_and_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "def merge_or_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            result.append(posting_term1[i])\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                result.append(posting_term2[j])\n",
    "                j=j+1\n",
    "    if i <n:\n",
    "        result = result + posting_term1[i:]\n",
    "    if j <m:\n",
    "        result = result + posting_term2[j:]\n",
    "    return result\n",
    "\n",
    "def merge_and_not_postings_list(posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    n = len(posting_term1)\n",
    "    m = len(posting_term2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < n and j <m:\n",
    "        if posting_term1[i] == posting_term2[j]:\n",
    "            i = i+1\n",
    "            j = j+1\n",
    "        else:\n",
    "            if posting_term1[i] < posting_term2[j]:\n",
    "                result.append(posting_term1[i])\n",
    "                i = i+1\n",
    "            else:\n",
    "                j=j+1\n",
    "    return result\n",
    "\n",
    "# généralise le merge selon l'opérateur\n",
    "def boolean_operator_processing_with_inverted_index(BoolOperator: str, posting_term1: list, posting_term2: list)->list:\n",
    "    result=[]\n",
    "    if BoolOperator == \"AND\":\n",
    "        result.append(merge_and_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator==\"OR\" :\n",
    "        result.append(merge_or_postings_list(posting_term1,posting_term2))\n",
    "    elif BoolOperator == \"NOT\":\n",
    "        result.append(merge_and_not_postings_list(posting_term1,posting_term2))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_boolean(query: str, inverted_index_simple: dict, booleanOperator=BooleanOperator):\n",
    "    evaluation_stack = []\n",
    "    # transformer query en liste de mots\n",
    "    query = extract_words(query)\n",
    "\n",
    "    for term in query:\n",
    "        if term.upper() not in booleanOperator:\n",
    "          evaluation_stack.append(inverted_index_simple[term])#on rajoute la posting list du dernier terme\n",
    "        else:\n",
    "            if term.upper() == \"NOT\":\n",
    "              operande= evaluation_stack.pop()\n",
    "              eval_prop = boolean_operator_processing_with_inverted_index(term.upper(), evaluation_stack.pop(),operande)\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "            else:\n",
    "              operator = term.upper()\n",
    "              eval_prop =  boolean_operator_processing_with_inverted_index(operator, evaluation_stack.pop(),evaluation_stack.pop())\n",
    "              evaluation_stack.append(eval_prop[0])\n",
    "    return  evaluation_stack.pop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilstic Search (OBM25)\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python \n",
    "search_OBM25(query: str, inverted_index: dict, simple_index: pd.DataFrame, top: int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilistic Search Okapi BM25\n",
    "\n",
    "def search_OBM25(query: str, inverted_index: dict =full_index,\n",
    "                 simple_index: pd.DataFrame =clean_posts, top: int =5):\n",
    "  #constantes \n",
    "  k1 = 1.2\n",
    "  k3 = 1000\n",
    "  b = 0.75\n",
    "  m = np.mean(simple_index['len']) #longueur moyenne des docs, à trouver\n",
    "  #traitement de la query\n",
    "  query_ind = {}\n",
    "  query_tmt = nltk.word_tokenize(query)\n",
    "\n",
    "  for i in range(len(query_tmt)) : \n",
    "    query_tmt[i] = lemmatizer.lemmatize(query_tmt[i])\n",
    "  for word in query_tmt:\n",
    "    tf = query_tmt.count(word)/len(query_tmt)\n",
    "    query_ind[word] = tf\n",
    "  \n",
    "  N = len(posts)\n",
    "  #CORE on va faire sum(a*b*c) sur les termes pour chaque doc\n",
    "  \n",
    "  RSV = {}\n",
    "\n",
    "  for word in query_ind.keys():\n",
    "    if word in inverted_index:\n",
    "      df_j = inverted_index[word]['df']\n",
    "      \n",
    "      tuple_list = inverted_index[word]['inv_ind']\n",
    "      tf_j_q = query_ind[word]\n",
    "      a3 = math.log((N-df_j+0.5)/df_j+0.5)\n",
    "      a2 = (k3 + 1 ) * tf_j_q / ( k3 + tf_j_q)\n",
    "      for tuple_elt in tuple_list : \n",
    "        (doc_id , tf_j_d) = tuple_elt\n",
    "        L = simple_index.loc[simple_index['Id'] == doc_id].iloc[0]['len']\n",
    "        a1 = (k1 + 1) * tf_j_d / (k1 * ((1-b) + b * L/m) + tf_j_d)\n",
    "        if not(doc_id in RSV) :\n",
    "          RSV[doc_id] = a1 * a2 *a3\n",
    "        else :\n",
    "          RSV[doc_id] += a1 *a2 * a3\n",
    "\n",
    "  RSV = sorted(RSV.items(), key=lambda x: x[1], reverse=True)\n",
    "  return RSV[0:top]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIB\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "search_MIB(quey: str, inverted_index: dict, top: int)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def search_MIB(query: str, inverted_index: dict =full_index, data=posts, top: int =5):\n",
    "  N = len(data) #nombre des posts\n",
    "  tokens = nltk.word_tokenize(query)\n",
    "  Docs_id = dict()\n",
    "  for i in range(len(tokens)):\n",
    "    tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "    if tokens[i] in inverted_index:\n",
    "      for j in range(len(inverted_index[tokens[i]]['inv_ind'])):\n",
    "        if inverted_index[tokens[i]]['inv_ind'][j][0] not in Docs_id:\n",
    "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] = np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
    "        else:\n",
    "          Docs_id[inverted_index[tokens[i]]['inv_ind'][j][0]] += np.log(N/inverted_index[tokens[i]]['df']) * (1 + inverted_index[tokens[i]]['inv_ind'][j][1])\n",
    "  sort_orders = sorted(Docs_id.items(), key=lambda x: x[1], reverse=True)\n",
    "  return sort_orders[0:top]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "vectorizer_search(query : str, data=posts, vectors=vectors, vectorizer=vectorizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(posts.Cleaned_post.values)\n",
    "vectors = vectorizer.transform(posts.Cleaned_post.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query(query: str)->str:\n",
    "    return query.lower()\n",
    "\n",
    "def vectorize_query(query : str, vectorizer=vectorizer):\n",
    "    \"\"\"Vectorizes the query\n",
    "    Args:\n",
    "        query (str): query string\n",
    "        vectorizer (optional): Defaults to vectorizer.\n",
    "\n",
    "    Returns:\n",
    "        query vectorized\n",
    "    \"\"\"\n",
    "    query_vectorized = vectorizer.transform([clean_query(query)])\n",
    "    return query_vectorized\n",
    "\n",
    "\n",
    "def search_tfidf(query : str,\n",
    "                 data=posts,\n",
    "                 vectors=vectors,\n",
    "                 vectorizer=vectorizer) -> list:\n",
    "    \"\"\" renvoit une liste de rankings (doc_id, score) \"\"\"\n",
    "    query_vectorized = vectorize_query(query, vectorizer) \n",
    "    results =  vectors @ query_vectorized.transpose()\n",
    "    res_dict = {}\n",
    "    doc_id_array = results.tocoo().row\n",
    "    for i in range(len(doc_id_array)):\n",
    "      res_dict[data.iloc[doc_id_array[i]]['Id']] = results.data[i]\n",
    "    return sorted(res_dict.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity\n",
    "\n",
    "La fonction à appeler est :\n",
    "```python\n",
    "search_semantic(query: str, data=posts, embeddings=embeddings, top: int =10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_model = 'multi-qa-mpnet-base-dot-v1'\n",
    "MODEL_ST = SentenceTransformer(sentence_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prend du temps à éxecuter (~15mins)\n",
    "embeddings = MODEL_ST.encode(posts.cleaned_body.values, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load the embeddings into pickle format\n",
    "def save_embedding(savepath, embeddings):\n",
    "    \"\"\" save the embedding into a pickle file\"\"\"\n",
    "    with open(savepath, 'wb') as file:\n",
    "        pickle.dump(embeddings, file)\n",
    "\n",
    "def load_embedding(savepath):\n",
    "    \"\"\" load embedding from a pickle file \"\"\"\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# save_embeddings(EMBEDDING_PATH, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = load_embedding(EMBEDDING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_query(query : str) ->  np.ndarray:\n",
    "    \"\"\" encode query with the embedding \"\"\"\n",
    "    encoded_query = MODEL_ST.encode(query)\n",
    "    return encoded_query\n",
    "\n",
    "\n",
    "def similarity(query: str, embeddings=embeddings):\n",
    "    \"\"\" renvoie la matrice de similarité entre la requête et les posts \"\"\"\n",
    "    query_emb = encode_query(query)\n",
    "    similarity_matrix = util.dot_score(query_emb, embeddings)[0].cpu().tolist()\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def ordre_en_fonction_similarité(matrix_similarity)->list[tuple]:\n",
    "    \"\"\" renvoie le classement des indices à partir de la matrice de similarité \"\"\"\n",
    "    scores = [(i, matrix_similarity[i]) for i in range(len(matrix_similarity))]\n",
    "    ordre = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return ordre \n",
    "\n",
    "\n",
    "def search_semantic(query: str, data=posts, embeddings=embeddings, top: int =10)->list[tuple]:\n",
    "    \"\"\" renvoie le classement des posts les plus similaires à la requête \"\"\"\n",
    "    sim_mat = similarity(query, embeddings)\n",
    "    sorted_indexes = ordre_en_fonction_similarité(sim_mat)\n",
    "    max_score = sorted_indexes[0][1]\n",
    "    closest_posts = [(data.Id.iloc[j], score / max_score) for (j, score) in sorted_indexes]\n",
    "    return closest_posts[0:top]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize document using TF-IDF\n",
    "vectorizer_lda = TfidfVectorizer()\n",
    "\n",
    "# Fit and Transform the documents\n",
    "train_data = vectorizer_lda.fit_transform(posts.Cleaned_post.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 200\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
    "lda_model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load the lda model into pickle format\n",
    "\n",
    "def save_ldamodel(savepath, lda_model):\n",
    "    \"\"\" save the lda_model into a pickle file\"\"\"\n",
    "    with open(savepath, 'wb') as file:\n",
    "        pickle.dump(lda_model, file)\n",
    "\n",
    "def load_ldamodel(savepath):\n",
    "    \"\"\" load the lda model from a pickle file\"\"\"\n",
    "    with open(savepath, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "# save_ldamodel(LDAMODEL_PATH, lda_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = load_ldamodel(LDAMODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_find(text):\n",
    "  \"\"\" renvoie le thème associé au texte en argument \"\"\"\n",
    "  new_document_vectorized = vectorizer_lda.transform([text])\n",
    "  topic_probabilities = lda_model.transform(new_document_vectorized)\n",
    "  dominant_topic = topic_probabilities.argmax()\n",
    "  return dominant_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['Topic'] = posts['Cleaned_post'].fillna('').apply(topic_find)\n",
    "posts['Words'] = posts['Cleaned_post'].fillna('').apply(remove_tags).apply(extract_words).apply(filter_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_query(text, vectorizer=vectorizer_lda, lda_model=lda_model) -> int:\n",
    "  \"\"\" renvoie le thème associé à la requête \"\"\"\n",
    "  new_document_vectorized = vectorizer.transform([text])\n",
    "  topic_probabilities = lda_model.transform(new_document_vectorized)\n",
    "  dominant_topic = topic_probabilities.argmax()\n",
    "  return dominant_topic\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged search algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_search_algorithm(query,\n",
    "                         ponderation = 1/6*np.ones((6)),\n",
    "                         data=posts,\n",
    "                         inverted_index=full_index,\n",
    "                         simple_index=clean_posts,\n",
    "                         # topic_documents=topic_documents,\n",
    "                         vectors=vectors,\n",
    "                         vectorizer=vectorizer,\n",
    "                         vectorizer_lda=vectorizer_lda,\n",
    "                         lda_model=lda_model,\n",
    "                         embeddings=embeddings,\n",
    "                         top_n=10\n",
    "                         )->list:\n",
    "    \"\"\" moteur de recherche issu de la fusion des modèles précédents \"\"\"\n",
    "    #Pondération\n",
    "    coefs = ponderation\n",
    "    \n",
    "    #Récupération de tous les rankings listes pour la query\n",
    "    rankings = []\n",
    "    tip_top = min(top_n, data.shape[0])\n",
    "\n",
    "    r_boolean = search_boolean(query, inverted_index_simple )\n",
    "    rankings.append(r_boolean)\n",
    "\n",
    "    #pour traiter le plus de documents pertinents possibles\n",
    "    tip_top = max(tip_top, len(r_boolean)) \n",
    "    \n",
    "    r_naive = search_naive(query, inverted_index, tip_top) \n",
    "    rankings.append(r_naive)\n",
    "\n",
    "    r_obm25 = search_OBM25(query, inverted_index , simple_index , tip_top)\n",
    "    rankings.append(r_obm25)\n",
    "\n",
    "    r_MIB = search_MIB(query, inverted_index, data, tip_top)\n",
    "    rankings.append(r_MIB)\n",
    "\n",
    "    r_tfidf = search_tfidf(query, data, vectors, vectorizer)\n",
    "    rankings.append(r_tfidf)\n",
    "\n",
    "    r_semantic = search_semantic(query, data, embeddings, tip_top)\n",
    "    rankings.append(r_semantic)\n",
    "\n",
    "    ranking_dict = {}\n",
    "\n",
    "    #On traite tous les modèles sauf le booléen\n",
    "    for i in range(1, len(rankings)) : \n",
    "      r_i = rankings[i]\n",
    "      for j in range(len(r_i)):\n",
    "        (doc_id, score_j) = r_i[j]\n",
    "        if doc_id not in ranking_dict : \n",
    "          ranking_dict[doc_id] = coefs[i] * score_j / log( j + 2 )\n",
    "        else : \n",
    "          ranking_dict[doc_id] += coefs[i] * score_j / log( j + 2 )\n",
    "\n",
    "    #On ajoute booléen, qui va pénaliser les docs où les termes sont absents\n",
    "    N = len(rankings[0])\n",
    "    for j in range( N ):\n",
    "        if doc_id not in ranking_dict : \n",
    "          ranking_dict[doc_id] = coefs[0] / N\n",
    "        else : \n",
    "          ranking_dict[doc_id] += coefs[0] / N\n",
    "\n",
    "    matching_posts = sorted(ranking_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return matching_posts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadonnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_score( ranking_list : list[tuple], c_score:float) -> list[tuple]:\n",
    "  \"\"\" A partir d'une liste de résultats d'une fonction de search, on utilise\n",
    "  le score de chaque posts pour affiner la pertinence, en faisant copter un poids c_score\"\"\"\n",
    "  meta_list = [] #list[tuple] [(doc_id, score)]\n",
    "  for couple in ranking_list:\n",
    "    meta_list.append((couple[0],posts[posts['Id'] == id]['Score'].values))\n",
    "  #On ordonne la liste des rankings par score\n",
    "  meta_ranking_list = sorted(meta_list, key=lambda x: x[1], reverse=True)\n",
    "  \n",
    "  #On utilise la structure de dictionnaire pour accéder facilement aux doc_id\n",
    "  new_ranking = {}\n",
    "  for cpl in ranking_list:\n",
    "    new_ranking[cpl[0]] = cpl[1] #new_ranking[doc_id] = score_init\n",
    "  #On normalise le poids grâce à l'élément de plus haut score\n",
    "  c2 = c_score * new_ranking[meta_ranking_list[0][0]] \n",
    "  N = len(meta_ranking_list)\n",
    "  \n",
    "  for j in range(N) : \n",
    "    new_ranking[meta_ranking_list[j][0]] += c2 *(N-j)/N \n",
    "  \n",
    "  return sorted(new_ranking.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(query, results):\n",
    "    print(\"La requête :\")\n",
    "    print(query)\n",
    "    print(\"Résultats :\")\n",
    "    for post_id, score in results:\n",
    "      id = posts[\"Id\"][posts[\"Id\"]==post_id].values[0]\n",
    "      print(\"Post Id : \", id)\n",
    "      title = posts[\"Title\"][posts[\"Id\"]==post_id].values[0]\n",
    "      print(\"Title : \", title)\n",
    "      print(\"First sentence :\")\n",
    "      body = posts[\"Body\"][posts[\"Id\"]==post_id].values[0]\n",
    "      first_sentence = body[0:150]\n",
    "      print(first_sentence)\n",
    "    return\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Relevancy CSV\n",
    "# /!\\ changer le filepath\n",
    "df_relevancy = pd.read_excel(os.path.join(DATA_PATH, \"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\"))\n",
    "df_relevancy = df_relevancy.fillna(0)\n",
    "df_relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = {1:'Query 1 : mesure performance for multiclassification model',\n",
    "                2:'Query 2 : draw neural network',\n",
    "                3:'Query 3 : neural network layers',\n",
    "                4:'Query 4 : how sklearn working',\n",
    "                5:'Query 5 : treat categorical data',\n",
    "                'Query 1 : mesure performance for multiclassification model': 1,\n",
    "                'Query 2 : draw neural network': 2,\n",
    "                'Query 3 : neural network layers': 3,\n",
    "                'Query 4 : how sklearn working': 4,\n",
    "                'Query 5 : treat categorical data': 5}\n",
    "\n",
    "def calc_dcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  \"\"\" calcule le dcg à partir du classement renvoyé par un modèle de recherche effectué sur le sous-ensemble de documents \"\"\"\n",
    "  dcg = 0\n",
    "  for k in range(rank):\n",
    "    id = query_results[k]\n",
    "    score = df_relevancy[test_queries[query_number]][df_relevancy[\"PostId\"]==id].iloc[0]/ (log(k+2)/log(2))\n",
    "    dcg +=  score \n",
    "  return dcg\n",
    "\n",
    "\n",
    "def calc_dcg_ideal(rank: int =5, query_number: int =1)->float:\n",
    "  dcg_ideal = 0\n",
    "  perfect_ranking = sorted(list(df_relevancy[test_queries[query_number]]), reverse=True)\n",
    "  for k in range(rank):\n",
    "    dcg_ideal += perfect_ranking[k] / log(k+2, 2)\n",
    "  return dcg_ideal\n",
    "\n",
    "\n",
    "def calculate_ndcg(query_results: list[int], rank: int =5, query_number: int =1)->float:\n",
    "  return calc_dcg(query_results, rank, query_number) / calc_dcg_ideal(rank, query_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dcg(query_number=2, maxrank=10):\n",
    "  \"\"\" permet d'afficher le dcg des différents modèles \"\"\"\n",
    "  def dcg_list(query_results, query_number=2, maxrank=5):\n",
    "    dcg_list = []\n",
    "    for k in range(maxrank):\n",
    "      id = query_results[k]\n",
    "      score = df_relevancy[test_queries[query_number]][df_relevancy[\"PostId\"]==id].iloc[0]/ (log(k+2)/log(2))\n",
    "      if not dcg_list == []:\n",
    "        score += dcg_list[-1]\n",
    "      dcg_list.append(score)\n",
    "    return dcg_list\n",
    "  query_2 = test_queries[query_number]\n",
    "\n",
    "  # naive_result = search_naive(query_2, subset_invind, top=10)\n",
    "  bm25_result = search_OBM25(query_2, subset_invind, subset_posts, top=10)\n",
    "  mib_result = search_MIB(query_2, subset_invind, subset_posts, 10)\n",
    "  tfidf_result = search_tfidf(query_2, subset_posts, subset_vectors, subset_vectorizer)\n",
    "  semantic_result = search_semantic(query_2, subset_posts, subset_embeddings, 10)\n",
    "  merged_result = nlp_search_algorithm(query_2, data=subset_posts, inverted_index=subset_invind, \n",
    "                                      vectors=subset_vectors, vectorizer=subset_vectorizer, \n",
    "                                      embeddings=subset_embeddings)\n",
    "\n",
    "  # naive_ranking = [r[0] for r in naive_result]\n",
    "  bm25_ranking  = [r[0] for r in bm25_result]\n",
    "  mib_ranking   = [r[0] for r in mib_result]\n",
    "  tfidf_ranking = [r[0] for r in tfidf_result]\n",
    "  semantic_ranking = [r[0] for r in semantic_result]\n",
    "  merged_ranking = [r[0] for r in merged_result]\n",
    "\n",
    "  # naive_dcg = dcg_list(naive_ranking)\n",
    "  bm25_dcg = dcg_list(bm25_ranking)\n",
    "  mib_dcg = dcg_list(mib_ranking)\n",
    "  tfidf_dcg = dcg_list(tfidf_ranking)\n",
    "  semantic_dcg = dcg_list(semantic_ranking)\n",
    "  merged_dcg = dcg_list(merged_ranking)\n",
    "\n",
    "  # plt.plot(naive_dcg, 'o-', label=\"Naive\")\n",
    "  plt.plot(bm25_dcg, 'o-', label=\"Okapi BM25\")\n",
    "  plt.plot(mib_dcg, 'o-', label=\"MIB\")\n",
    "  plt.plot(tfidf_dcg, 'o-', label=\"Tf-Idf\")\n",
    "  plt.plot(semantic_dcg, 'o-',  label=\"Semantic\")\n",
    "  plt.plot(merged_dcg, 'o-', label=\"Fusion\")\n",
    "\n",
    "  plt.xlabel(\"rank\")\n",
    "  plt.ylabel(\"DCG\")\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS on the subset of documents which have been judged by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_docs = set(df_relevancy[\"PostId\"])\n",
    "subset_posts = clean_posts[clean_posts['Id'].isin(subset_docs)]\n",
    "subset_posts['Cleaned_post'] = subset_posts['Body'].fillna('').apply(clean_post)\n",
    "subset_invind = create_inverted_index(subset_posts)\n",
    "subset_vectorizer = TfidfVectorizer()\n",
    "subset_vectorizer.fit(subset_posts.Cleaned_post.values)\n",
    "subset_vectors = subset_vectorizer.transform(subset_posts.Cleaned_post.values)\n",
    "subset_embeddings = MODEL_ST.encode(subset_posts.Cleaned_post.values, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ndcg for our previous search models\n",
    "\n",
    "query_1 = 'mesure performance for multiclassification model'\n",
    "query_2 = 'draw neural network'\n",
    "naive_result = search_naive(query_2, subset_invind, 10)\n",
    "bm25_result = search_OBM25(query_2, subset_invind, subset_posts, top=10)\n",
    "mib_result = search_MIB(query_2, subset_invind, subset_posts, 10)\n",
    "tfidf_result = search_tfidf(query_2, subset_posts, subset_vectors, subset_vectorizer)\n",
    "semantic_result = search_semantic(query_2, subset_posts, subset_embeddings, 10)\n",
    "merged_result = nlp_search_algorithm(query_2, data=subset_posts, inverted_index=subset_invind, \n",
    "                                     vectors=subset_vectors, vectorizer=subset_vectorizer, \n",
    "                                     embeddings=subset_embeddings)\n",
    "\n",
    "naive_ranking = [r[0] for r in naive_result]\n",
    "bm25_ranking  = [r[0] for r in bm25_result]\n",
    "mib_ranking   = [r[0] for r in mib_result]\n",
    "tfidf_ranking = [r[0] for r in tfidf_result]\n",
    "semantic_ranking = [r[0] for r in semantic_result]\n",
    "merged_ranking = [r[0] for r in merged_result]\n",
    "\n",
    "print(calculate_ndcg(naive_ranking, query_number=2))\n",
    "print(calculate_ndcg(bm25_ranking, query_number=2))\n",
    "print(calculate_ndcg(mib_ranking, query_number=2))\n",
    "print(calculate_ndcg(tfidf_ranking, query_number=2))\n",
    "print(calculate_ndcg(semantic_ranking, query_number=2))\n",
    "print(calculate_ndcg(merged_ranking, query_number=2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouverture (tentatives de code non aboutis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a better ponderation (not yet functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_pond(nb_models=6):\n",
    "  pond = [np.random.random() for _ in range(nb_models)]\n",
    "  s = sum(pond)\n",
    "  return [i/s for i in pond]\n",
    "\n",
    "def get_imax(liste):\n",
    "  i_max = 0\n",
    "  for j in range(1, len(liste)):\n",
    "    if liste[i_max] < liste[j]:\n",
    "      i_max = j\n",
    "  return i_max\n",
    "\n",
    "def find_ponderation(query_number=2, nb_reps=10, nb_models=6):\n",
    "  to_test = [rand_pond(nb_models) for _ in range(nb_reps)]\n",
    "  ndcg_list = []\n",
    "  for pond in to_test:\n",
    "    query = test_queries[query_number]\n",
    "    results = nlp_search_algorithm(query, ponderation=pond, data=subset_posts, inverted_index=subset_invind,\n",
    "                     simple_index=clean_posts,\n",
    "                     vectors=subset_vectors,\n",
    "                     vectorizer=subset_vectorizer,\n",
    "                     embeddings=subset_embeddings)\n",
    "    ndcg_list.append(calculate_ndcg(results, query_number))\n",
    "  i_max = get_imax(ndcg_list)\n",
    "  return to_test[i_max]\n",
    "\n",
    "\n",
    "p = find_ponderation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche par contexte (à partir d'un réseau de neurones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation: on note docs(t_i) la liste des documents jugés pertinents suite à la recherche du term_i par search engine.\n",
    "\n",
    "\n",
    "But : En utilisant le réseau de neurones word2vec skip gram, trouver les mots liés à t_i dans le contexte donné, et renvoyer docs(t_i) inter docs(t_j) au lieu de simplement docs(t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "\t\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "\te_x = np.exp(x - np.max(x))\n",
    "\treturn e_x / e_x.sum()\n",
    "\n",
    "class word2vec(object):\n",
    "\tdef __init__(self):\n",
    "\t\tself.N = 10\n",
    "\t\tself.X_train = []\n",
    "\t\tself.y_train = []\n",
    "\t\tself.window_size = 2\n",
    "\t\tself.alpha = 0.001\n",
    "\t\tself.words = []\n",
    "\t\tself.word_index = {}\n",
    "\n",
    "\tdef initialize(self,V,data):\n",
    "\t\tself.V = V\n",
    "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "\t\t\n",
    "\t\tself.words = data\n",
    "\t\tfor i in range(len(data)):\n",
    "\t\t\tself.word_index[data[i]] = i\n",
    "\n",
    "\t\n",
    "\tdef feed_forward(self,X):\n",
    "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
    "\t\tself.u = np.dot(self.W1.T,self.h)\n",
    "\t\t#print(self.u)\n",
    "\t\tself.y = softmax(self.u)\n",
    "\t\treturn self.y\n",
    "\t\t\n",
    "\tdef backpropagate(self,x,t):\n",
    "\t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
    "\t\t# e.shape is V x 1\n",
    "\t\tdLdW1 = np.dot(self.h,e.T)\n",
    "\t\tX = np.array(x).reshape(self.V,1)\n",
    "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
    "\t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
    "\t\tself.W = self.W - self.alpha*dLdW\n",
    "\t\t\n",
    "\tdef train(self,epochs):\n",
    "\t\tfor x in range(1,epochs):\t\n",
    "\t\t\tself.loss = 0\n",
    "\t\t\tfor j in range(len(self.X_train)):\n",
    "\t\t\t\tself.feed_forward(self.X_train[j])\n",
    "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
    "\t\t\t\tC = 0\n",
    "\t\t\t\tfor m in range(self.V):\n",
    "\t\t\t\t\tif(self.y_train[j][m]):\n",
    "\t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
    "\t\t\t\t\t\tC += 1\n",
    "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
    "\t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
    "\t\t\t\n",
    "\tdef predict(self,word,number_of_predictions):\n",
    "\t\tif word in self.words:\n",
    "\t\t\tindex = self.word_index[word]\n",
    "\t\t\tX = [0 for i in range(self.V)]\n",
    "\t\t\tX[index] = 1\n",
    "\t\t\tprediction = self.feed_forward(X)\n",
    "\t\t\toutput = {}\n",
    "\t\t\tfor i in range(self.V):\n",
    "\t\t\t\toutput[prediction[i][0]] = i\n",
    "\t\t\t\n",
    "\t\t\ttop_context_words = []\n",
    "\t\t\tfor k in sorted(output,reverse=True):\n",
    "\t\t\t\ttop_context_words.append(self.words[output[k]])\n",
    "\t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
    "\t\t\t\t\tbreak\n",
    "\t\n",
    "\t\t\treturn top_context_words\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Word not found in dictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "\t\"\"\" pré-traite les mots du corpus \"\"\"\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttraining_data = []\n",
    "\tsentences = corpus.split(\".\")\n",
    "\tfor i in range(len(sentences)):\n",
    "\t\tsentences[i] = sentences[i].strip()\n",
    "\t\tsentence = sentences[i].split()\n",
    "\t\tx = [word.strip(string.punctuation) for word in sentence\n",
    "\t\t\t\t\t\t\t\t\tif word not in stop_words]\n",
    "\t\tx = [word.lower() for word in x]\n",
    "\t\ttraining_data.append(x)\n",
    "\treturn training_data\n",
    "\t\n",
    "\n",
    "def prepare_data_for_training(sentences,w2v):\n",
    "\tdata = {}\n",
    "\tfor sentence in sentences:\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tif word not in data:\n",
    "\t\t\t\tdata[word] = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tdata[word] += 1\n",
    "\tV = len(data)\n",
    "\tdata = sorted(list(data.keys()))\n",
    "\tvocab = {}\n",
    "\tfor i in range(len(data)):\n",
    "\t\tvocab[data[i]] = i\n",
    "\t\n",
    "\t#for i in range(len(words)):\n",
    "\tfor sentence in sentences:\n",
    "\t\tfor i in range(len(sentence)):\n",
    "\t\t\tcenter_word = [0 for x in range(V)]\n",
    "\t\t\tcenter_word[vocab[sentence[i]]] = 1\n",
    "\t\t\tcontext = [0 for x in range(V)]\n",
    "\t\t\t\n",
    "\t\t\tfor j in range(i-w2v.window_size,i+w2v.window_size):\n",
    "\t\t\t\tif i!=j and j>=0 and j<len(sentence):\n",
    "\t\t\t\t\tcontext[vocab[sentence[j]]] += 1\n",
    "\t\t\tw2v.X_train.append(center_word)\n",
    "\t\t\tw2v.y_train.append(context)\n",
    "\tw2v.initialize(V,data)\n",
    "\n",
    "\treturn w2v.X_train,w2v.y_train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But ici: bien l'entrainer le réseau de neurones récuper 1 posts sur 10 et lui donner des données\n",
    "\n",
    "mode d'emploi: ``recup1_10()`` permet de nourrire le corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing pour le petit réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
    "bodies=posts['Body']\n",
    "corpus=''\n",
    "def recup1_10():\n",
    "\n",
    "  res=''#futur nourriture pour la petit réseau de neurones\n",
    "  c=np.random.randint(0,10)\n",
    "  for text in bodies:\n",
    "    if c==0:\n",
    "      res = text + ' '\n",
    "      c+=1\n",
    "    else:\n",
    "      c+=1\n",
    "  corpus = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "training_data = preprocessing(corpus)\n",
    "w2v = word2vec()\n",
    "\n",
    "prepare_data_for_training(training_data,w2v)\n",
    "w2v.train(epochs)\n",
    "\n",
    "#w2v.predict(\"around\",1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation du mot_contexte dans le search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indice(element, liste):\n",
    "    try:\n",
    "        indice = liste.index(element)\n",
    "        return indice\n",
    "    except ValueError:\n",
    "        return None\n",
    "def merge_rank(l1,l2): #l1=classement des docs pertinents pour le terme de la requête, l2 pour le mot mot contexte\n",
    "  final_list=[]\n",
    "  for i1 in range(len(l1)):\n",
    "    if l1[i1] not in l2:\n",
    "      final_list.append((l1[i1],0.8*(i1+1)+0.2*5))#on le pénalise de ne pas être dans la liste pertinents liés au mot contexte\n",
    "    elif l1[i1] in l2:\n",
    "      i2=get_indice(l1[i1],l2)\n",
    "      final_list.append((0.8*(i1+1)+0.2*(i2+1)))#les rank apparents sont bien flottants mais pas grave\n",
    "  for i2 in range(len(l2)):\n",
    "    if l2[i2] not in l1:\n",
    "      final_list.append((l2[i2],0.8*10+0.2*i2))\n",
    "  sorted(final_list,key=lambda x:x[1])#on trie par rang\n",
    "  return final_list#liste des docs_id\n",
    "\n",
    "#But=faire fonction qui permet d'utiliser aussi le mot_context\n",
    "def final_search_engine(query):\n",
    "  for term in query:\n",
    "    mot_context=w2v.predict('term')[0]\n",
    "    l1=search_engine(term)\n",
    "    if not mot_context in query:\n",
    "      l2=search_engine(mot_context)\n",
    "      merge_rank(l1,l2)#mais il reste la question de comment on marge les final_list entre mots de la query\n",
    "    else:\n",
    "      return l1\n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a query - Try it out !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "query_demo = 'enter query here'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged search engine\n",
    "results_merged = nlp_search_algorithm(query_demo)\n",
    "visualize_output(query_demo, results_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add-on with the metadata\n",
    "metadata_weight = 0.1\n",
    "results_after_metadata = metadata_score(results_demo, metadata_weight)\n",
    "visualize_output(query_demo, results_after_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "topic_query = get_topic_query(query_demo)\n",
    "posts_with_topic = posts.loc[posts['Topic'] == topic_query]\n",
    "full_ind_k = create_inverted_index(posts_with_topic)\n",
    "results_cluster = nlp_search_algorithm(query_demo, inverted_index=full_ind_k)\n",
    "visualize_output(query_demo, results_cluster)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
